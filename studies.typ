#import "glossary.typ": *

= Studies

== Enhancing scenarios with interactive chatbots

The study aims to make interactive chatbots for cyber simulations more capable and realistic using deep learning and LLMs; and to upgrade it to handle more complex scenarios. It shall be given capabilities that allow it to act as participants in exercises similar to table top role playing only with integration of real technology as well. It will also be used in LLM security exercises that involve manipulating AIs to get responses from the prompt. The study will involve adding features to Hackerbot, including the ability to use LLMs to generate responses. Prompts, models, and other parameters will be configurable through files generated by SecGen.

Development and testing of artifacts for the hackerbot study is already well underway with a functional implementation. It shall be live tested in the coming semester on students in incident response, and the next semester with systems security.

It is also important to determine which LLM is most suitable to use. This will need to be based on criteria such as the quality of the output, the time it takes to generate responses, and the size and resource consumption of the model. In order to evaluate response quality metrics will be needed. Some of these can be existing benchmarks of LLMs, but for the purpose of this study there will also need to be metrics for this specific application. A small study may be needed to compare the different LLMs that meet the resource criteria. Some candidate models for this include Qwen3 1.5B parameter, TinyLLaMa 1B parameter, DeepSeek distilled 1B parameter, and Gemma3 1B parameter. It may also be worth exploring different quantized versions of the model, as quantization significantly reduces memory requirements at the cost of output quality. Model loading parameters such as context length will also need to be determined, as this can significantly effect both model performance and memory requirements. There are technologies such as Flash Attention and KV quantization that will also be considered.

For the purposes of this research area an existing chatbot will be used as a starting point; specifically hackerbot will be used. Hackerbot is a piece of software currently used within the learning platform Hacktivity. It performs the role of acting as an attacker and an assistant to students. It is responsible for prompting them with tasks to perform. It currently uses a mixture of canned responses to commands, and a simple chatbot that is not based on any deep learning technique. This means that while it's a capable tool it has limitations and isn't particularly realistic to how a real threat actor might behave. It also limits it's use in role-play scenarios.

Different models will have to be compared to determine which are the most suitable for the task at hand. This will involve looking at existing model benchmarks for things such as instruction following and technical accuracy. It will also involve testing and evaluating the models responses manually. Prompts used by the software will need to be determined as part of this evaluation.

Techniques like TransMLA, quantization, and fine tuning could be used to increase model speed and output quality. These may require further traning on a GPU server or GPU cluster.

+ Develop software
+ Determine best model to use
+ Give software to students for them to test
+ Record any technical issues
+ Take surveys of their experience
+ Analyse results

== Scenario Generation

=== Generating narrative content and instructions

#acrshort("llm")s and image generation models will be used to create realistic companies, characters, names, and stories called narrative content for the randomized scenarios being generated. As part of this the models will need to be fed a precise prompt along with details about the lab. Additionally the language model might be used to generate prompts and characters for the hackerbot model to follow when interacting with the student.

=== Malware generation and obfuscation

The plan here is to use uncensored #acrshort("llm")s to create randomized malware for students to study. This is done through two techniques: one is to take existing working malware and use #acrshort("llm") to obscure it's source code by rewriting it. The other is to generate novel malware using the #acrshort("llm")s coding capabilities. In order to do this MCP servers may be used to provide additional documentation to the AI, along with using agentic techniques to help it write, debug, and update the code. This could be implemented using existing coding software such as OpenCode. Part of this process will involve testing the malware to make sure it works. This could involve automated and/or manual tests. Generating malware for students to analyse using #acrshort("llm")s is not something that has been done before.

=== Generating insecure software and system configurations

Along with the ability to create software LLMs can be used to produce #acrfull("iac") files and commands used to configure a system. The idea here is to use an #acrshort("llm") to generate part or all of a lab including the vulnerable software used or the insecure configuration. This would allow for novel insecure labs to be generated without needing extensive configuration with XML or complicated software for randomization. It would allow for a greater variety of labs and challenges to be generated than would otherwise be possible while reducing the amount of time spent by staff on developing new labs.

This will be the most challenging part of this PhD for current machine learning technology to accomplish. It is likely to require the use of advanced frontier models and/or finetuned models for this specific purpose. As part of this process the labs generated will need to be tested either automatically or manually to make sure they are both built correctly and are indeed vulnerable. Part of this could involve asking the model to generate a solution script designed to exploit the vulnerability in the lab and retrieve any flags therein. An automated testing system could then use this solution on an instance of the generated lab to ensure it is vulnerable. If the lab is found to not be solvable by the script then the model could be prompted to fix the lab, or a human signalled for manual intervention. This could involve a separate LLM agent configured specifically for debugging broken labs.