#import "glossary.typ": *

= Literature review

== Education and simulations

=== Authentic learning

Authentic learning is based on the concept of learning by doing. This can be done by solving real-world problems in simulated or actual environments. Technology is used to support authentic learning, such as computer simulations, VR, and visualization tools. Computers can allow for visualizing abstract concepts such as invisible physical fields and interactions between particles. Authentic learning also requires collaboration. Authentic learning is similar to how apprenticeships worked in the past. Authentic learning allow students to develop skills such as working with ill-defined problems, conducting sustained investigations, building finished products, building reflection and metacognitive skills, and doing all this in a way that's relevant to the real world. #cite(<lombardi_authentic_2007>). Authentic learning is one variety of constructivism.

=== Simulations

There are many goals for cybersecurity simulations. These include evaluating the impact of attacks and resilience of systems, testing defensive countermeasures, training machine learning models with synthetic data, and discovering vulnerabilities. There are a variety of techniques used in these simulations. These include agent-based modelling, petri-nets, equation-based modeling, probabilistic modeling, and hardware based simulation.#cite(<serena_simulation_2025>)

There is a system for cyber security excercises called Hacktivity. This system uses an #acrfull("apg") system called SecGen. This deterministically generates randomized scenarios similar to how some video games randomly generate maps. This includes the generation of flags using wordlists; generating flags for each user helps reduce the risk of academic misconduct. It is hosted on ovirt based infrastructure. One feature of this system is something called hackerbot. Hackerbot is designed to teach defensive cyber security skills by acting as an attacker, and giving out flags for successful defence of computer systems from automated hacking attempts. This addresses a problem with #acrshort("ctf")s which work better for offensive security than defensive security. #cite(<schreuders_generating_2015>) #cite(<schreuders_hackerbot_2018>) #cite(<schreuders_open_nodate>)

#acrfull("aicef") is a machine learning system which is used to generate cybersecurity exercise scenarios. Normally creating scenarios requires significant expertise and access to resources that most organizations do not posses. The goal here is to allow scenarios to be created with minimal expertise from unstructured information. It was found that this method generated scenarios of comparable quality to expert generated scenarios while being 33% faster to generate. While this is in some ways more advanced than SecGen, it is employing older machine learning techniques and models such as GPT-2.

UNIWA is a container-based cyber range platform. It uses docker containers within an OpenStack infrastructure. The #acrfull("iac") methodology is employed to manage the containers using Ansible and Heat orchestration templates. This aims to reduce cost and increase scalability as traditional cyber range platforms are not very scalable, with lightweight alternatives not having full isolation and orchestration capabilities. This container based approach is able to reduce execution time by 79%, memory by 90%, CPU usage by 50% and has reduced resource needs as workload scales. #cite(<chouliaras_novel_2023>) There are other container based cyber security excercise systems like CyExec created for efficiency and scalability reasons. #cite(<nakata_cyexec_2021>)

CyExec\* is a system that employs both containers and #acrfull("apg"). It uses a technique called a directed acylic graph to generate randomized scenarios. It is an extension/improvement upon the CyExec system mentioned earlier. This is a machine learning technique that relies on graph theory with connections between nodes that only move in one direction. As part of testing container, virtual machines, and real machines techniques were developed for converting a real machines software to virtual machine and containers. It was found that containers were the most efficient way of running cybersecurity scenarios, with virtual machines still being better than using real machine. They did however also find that certain vulnerabilities would not work the same way in a container or a virtual machine, but that these made up a small minority of vulnerabilities. #cite(<nakata_cyexec_2021>) #cite(<noauthor_pdf_2024>)

== #acrlong("ai") and #acrlong("llm")s

Modern #acrshort("llm")s are typically made up of transformer variants such as #acrfull("gpt"). These replaced older language model types such as #acrfull("lstm") models which are a type of #acrfull("rnn"). Transformer models were introduced in the paper #cite(<vaswani_attention_2023>) by engineers at Google. These had better scalability and performance than the existing #acrlong("rnn") based approaches. One of the key features of transformers is the #acrfull("mha") mechanism. In some newer models this has evolved into techniques such as #acrfull("mla") and #acrfull("gqa") which are less computationally intensive. This thesis will be mainly looking at these transformer based #acrshort("llm")s, though other kinds of #acrshort("llm")s such as #acrfull("llada") based models could be used. #acrshort("llada") is a novel kind of #acrshort("llm") which makes use of diffusion similar to image generation models such as stable diffusion.

Most #acrlong("dl") models are both trained and inferenced on #acrfull("gpu") using #acrfull("gpgpu") techniques using #acrfull("api") such as #acrfull("cuda"), #acrfull("opencl"), and Vulkan. Some are also run on #acrfull("npu") which are processors specialised on doing training and inferencing for deep learning models. Before the advent of this technique #acrlong("dl") models were run on #acrfull("cpu") hardware instead. This project will involve running #acrlong("dl") on both #acrshort("cpu")s and #acrshort("gpu")s.

Some LLMs that will be discussed in this section:

+ Gemma 3
+ DeepSeek R1
+ DeepSeek
+ DeepSeek V2
+ DeepSeek V3
+ DeepSeek V3.1
+ Mixtral
+ Qwen 3
+ GPT OSS
+ GLM-4.5

A new kind of scaling method has been developed for #acrshort("llm")s called parallel scaling. Rather than using parameter scaling (making larger dense or MoE models), or inference time scaling (reasoning models), this instead uses one set of parameters being executed in parallel and the results combined together. By creating a model using this method, they were able to come up with something that requires a fraction of the memory than a similar performing dense model while having improved latency. The table below is a copy of the table from this paper showing the advantages and disadvantages of different LLM scaling methods. The paper also contains many graphs showing the relationship between parameter scaling, parallel scaling, loss function, latency, and memory usage. This all suggests that parallel scaling can achieve similar or greater reductions in loss function as increasing the number of paramters while having lower memory and latency cost. The bulk of memory used in LLMs is for model parameters. Although running multiple instances of the model requires a larger KV cache in proportion to the number of streams used, this is only a fraction of the memory required for the model and so the increase in memory usage with parallel scaling is relatively small. Since model parameters are reused in each stream it could be said that the model is more efficient. #cite(<chen_parallel_2025>)

There is a hypothesis that increased model size improves memory while increased computation gives better reasoning capabilities. When tested on a variety of tasks it was found that parallel scaling had more impact on programming tasks than it did on general tasks. This supports the above hypothesis as the general tasks tested relied on memorization rather than reasoning.

Measuring the flops required for an #acrshort("llm") task is not a good way to measure it's inference requirements or performance. This is because many #acrshort("llm") tasks are bottlenecked by memory rather than by computation. An architecture can use more computation while taking a similar amount of time due to the memory limitations. This is why #cite(<chen_parallel_2025>) uses latency and memory usage as the metrics for #acrshort("llm") interference performance. In this model memory usage determines the minimum hardware requirements, and latency measures the time from input to output.

#align(center)[
  *Table: Copy of comparison table from #cite(<chen_parallel_2025>)*
]

#table(
  columns: 5,
  align: center,
  [*Method*], [*Inference Time*], [*Inference Space*], [*Training Cost*], [*Specialized Strategy*],
  [Dense Scaling], [üòê Moderate], [üò† High], [üò† Pre-training only], [üòä No],
  [MoE Scaling], [üòä Low], [üò† High], [üò† Pre-training only], [üò† Load balancing],
  [Inference-Time Scaling], [üò† High], [üòê Moderate], [üòä Post-training], [üò† RL / reward data],
  [Parallel Scaling], [üòê Moderate], [üòê Moderate], [üòä Pre- or Post-training], [üòä No],
)

#cite(<chen_parallel_2025>) also tells us about how parameter scaling and parallel scaling compare in terms of loss function, latency, and memory cost. This has the opportunity to answer an old question about machine learning, and that's if parameters or computation are responsible for larger models performing better than smaller ones. Conventionally with dense scaling the two have increased with each other in a proportional manner. With Parallel scaling the number of parameters stays the same, but the number of computations increase. The fact that parallel scaling significantly decreases loss function indicates that increasing computation is a significant factor in model performance, and suggests that's a reason larger models traditionally perform better.

There is a technique using multiple #acrshort("llm") calls to improve performance. It is found that while some problems benefit from more #acrshort("llm") calls others do not. These problems are thus categorized into easy and hard problems. Hard problems are ones that cannot be solved with an infinite number of calls to the model, and do not improve with more calls. Easy problems are those which can be answered better with more calls. Since benchmarks often have both kinds of problems this can lead to a non-monotonic scaling behaviour where the relationship between calls made and performance in benchmarks flips, such as more calls initially improving performance, then degrading it as calls further increase, or vice versa. The paper goes onto explore the proportions of hard and easy problems a benchmark or task must contain in order to exhibit this non-monotonic behaviour. It also suggests heuristics that could be used to estimate the optimum number of #acrshort("llm") calls to solve a given problem using only a small number of queries. #cite(<chen_are_2024>)

There is a theory that states all computable LLMs will hallucinate when asked certain questions. This is partly because they have a finite computation, and some answers are not solvable the amount of computation an LLM can perform. The kinds of problems that are guaranteed to cause hallucinations depend upon the computational complexity of the LLM. For any problem that has a computational complexity that scales faster than the #acrshort("llm") will be unsolvable by that LLM. Likewise they will attempt to solve problems that do not have a concrete solution and so will hallucinate that way. There are some suggestions on how to reduce or mitigate hallucinations in #acrshort("llm")s. #cite(<xu_hallucination_2025>)

=== Non-transformer based language models

As #acrshort("ai") and #acrshort("llm")s in particular have evolved several different architectures. The most common is based on something called a #acrshort("gpt") that use a basic unit called a Transformer. However there are other architectures that do not use transformers. These will be discussed here.

There are #acrshort("rnn")s using a #acrfull("lru"). This means that instead of using a neural network that takes a recurrent value and an input vector as input, and outputs an output vector and a new recurrent value, we instead use a linear function here. Linear functions are much cheaper to compute than multi-layer neural networks. We can then use a neural network on the output of the linear function. What this does is move the depedancy in the network which makes it more parallelizable. In a conventional recurrent neural network each neural net cannot be inferenced until the previous recurrent value has been calculated, and running a neural network is time consuming. With this new design the neural networks can be run in parallel as they are only dependant on the output of the linear function in the previous layer. These compete favorably with early state space models like S4 and S5.#cite(<orvieto_resurrecting_2023>)

There are some non-transformer based models. Mamba and liquid neural networks are two examples. Liquid neural networks are used in models such as LFM2. They are a new varient on #acrshort("rnn")s. Meanwhile MAMBA is based on both state space models and #acrfull("rnn")s. State space models are a method in mathematics used to model the states of complex systems. MAMBA is designed to surpass the #acrshort("rnn")s using the #acrshort("lru") #cite(<gu_mamba_2024>) #cite(<noauthor_introducing_nodate>)

=== Different attention mechanisms

The conventional multi-headed attention mechanism used by LLMs such as GPT-3 has a quadractic compute complexity, and requires significant of memory for a KV cache to make computation resonable. This means there is great interest in improving the computational and memory efficiency of LLMs. This section will describe a variety of different mechanisms that have been developed to overcome this, including new variations on the self-attention mechanism.

One is called #acrfull("mqa") and relies on using only a single key value pair for each attention head, where only the query value is kept independent per attention head. This increases inference speed and reduces memory bandwidth requirements and inference costs. It does so at a small loss in output quality. #cite(<shazeer_fast_2019>). #acrfull("gqa") is another varient built around a similar concept as #acrfull("mqa"). This is designed as a compromise between traditional #acrshort("mha") and #acrshort("mqa") mechanisms. Traditional #acrlong("mha") has separate keys and values for each query and attention head. This produces good output quality but requries significant computation to store all the keys and values. #acrshort("mqa") stores only a single key value pair for all queries and attention heads, which decreases needed memory at the cost of output quality. #acrshort("gqa") then batches some number of the queries together and uses a single key value pair for each group. This requires storing more keys and values than #acrshort("gqa") but has better output quality. The paper also states a technique to convert existing #acrshort("mha") models to #acrshort("gqa") by averaging query values. A varient called GQA-8 allows for performance similar to #acrshort("mha") with inference speed similar to #acrshort("mqa"). #cite(<ainslie_gqa_2023>)

DeepSeek V2 is the first model to use a new attention mechanism called Multi-Head Latent Attention. It also uses the #acrfull("moe") technique. This technique allows for better output quality than conventional #acrlong("mha") and #acrlong("gqa") techniques allow for, while using less memory. With DeepSeek-V2 they were able to achieve faster generation, using less computation than the original DeepSeek model despite having a larger parameter count (236B vs 67B) and greater output quality. #cite(<deepseek-ai_deepseek_2024>) #cite(<deepseek-ai_deepseek-v2_2024>) This increase in memory and computational efficiency means they can train and inference models at lower budgets than competitors as described in #cite(<deepseek-ai_deepseek-v3_2025>). #cite(<deepseek-ai_deepseek-v3_2025>) describes a newer model, DeepSeek V3 with 670B parameters and is based on the same techniques as DeepSeek-V2 but scaled up for better performance. This mechanism works by compressing down the keys and values into a combined latent space. TransMLA is a technique that can be applied to models originally built using the #acrshort("gqa") mechanism such as #acrshort("llama"), Qwen, and Mistral to convert them to #acrshort("mla"). This allows for the new converted versions of the models to inference significantly faster with only a small drop in output quality (less than 2%). #cite(<meng_transmla_2025>)

Kimi K2 is a new AI model released by MoonshotAI. Kimi K2 uses #acrlong("mla") much like the DeepSeek models. Unlike these however it uses some new training techniques. One of these is called MuonClip which allows for faster gradient decent during pre-training. It also uses some different reinforcement learning techniques to improve tool use, maths, logic, and coding performance by testing the answers the LLM generates during training inside of a sandbox environment to determine their accuracy, this is then given as feedback to the LLM. This process is called verifiable rewards. It also uses an adversarial process called the self-critique rubric where one instance of the model is used to judge and provide feedback to another based on a grading rubric. #cite(<team_kimi_2025>)

TransNormerLLM is the first linear attention based LLM. They benchmarked 7B, 1B, and 385M parameter variants while also creating a 175B parameter model. They found the small models to be comparable to other models of a similar size. #cite(<qin_transnormerllm_2023>). There are a pair of LLMs called MiniMax 01 and MiniMax M1 using a new form of hybrid attention system that combines lighting attention with normal attention blocks. This model is less computationally intensive for a given context than Qwen 3 235B despite being larger and having more active parameters, and is also faster than DeepSeek R1 0528. It also has a large context windows of 1 million tokens, 8 times larger than DeepSeek R1. #cite(<minimax_minimax-01_2025>) #cite(<minimax_minimax-m1_2025>)

=== Image generation models
#cite(<sordo_synthetic_2025>) says that the 3 main kinds of image generating models are #acrfull("vae"), #acrfull("gan"), and diffusion models. Currently Diffusion models are the most used, though transformer based DALL-E is also a contender.

=== Fine Tuning models
#cite(<ziegler_fine-tuning_2020>) talks about using human feedback to fine tune models for better results. It does this specifically using reinforcement learning technqiues. This is done by having the model generate multiple responses and having people rate those responses. This lead to some unfortunate consequences such as copying behavior and overfitting.

#cite(<houlsby_parameter-efficient_2019>) is a technique for more efficiently fine tuning language models. Instead of fine tuning on the whole model, a small trainable bottleneck module is trained on instead. This requires only 0.5% to 8% of the parameters be trained on. This gives almost the same performance as full model fine tuning (80.0 vs 80.4). #cite(<hu_lora_2021>) is a paper exploring a variation of the adapters technqiue called Low Rank Adapters. This has futher been extended to #cite(<dettmers_qlora_2023>) which is a quantized version of the LoRA technique used on quantized model. This allows for fine tuning on systems with less VRAM. For example a 65B parameter model can be fine tuned on a single 48GB GPU.

=== #acrfull("rag") and #acrfull("cag")
#acrshort("rag") is a technique that allows an LLM to access external data. This is done by having the LLM send the data to another model. This is converted to an embedding/vector. The embedding model uses this to search through a index of a knowledge base. This index also contains vectors that are compared against the vectors of the query. The retrieved information if then sent back to the LLM to generate a response. #acrshort("rag") could be used in a variety of applications including helping lawyers, doctors, and financial analysts with questions. #cite(<merritt_what_2025>).

LightRAG is an implementation of #acrshort("rag") that uses graph structures. This usese #acrshort("llm")s to extract entites and relationships from chunks of text. It uses these to make a knowledge graph to capture dependencies between entites. It then applies de-duplication and profiling to optimize graph operations. It supports updating this graph incrementally on the fly avoiding the need to rebuild the entire thing. #cite(<shan_learnrag_2025>)

LearnRAG is another open source RAG system. This one is designed for producing educational content. The retriever portion uses a hybrid approach combining high speed keyword based methods, with semantic techniques based on neural embeddings. The generator part uses models such as GPT-4 and T5 to produce coherent, accurate, and personalized education content for students using retrieved documents and the students profile. It has an indexing module, orchestration layer and a UI. The orchestration layer uses microservices principle to manage the other components.#cite(<shan_learnrag_2025>)

#acrfull("cag") is an alternative technique to #acrfull("rag"). This uploads all relevant data into the #acrshort("llm")s context window, then precomputes the KV cache which is stored. When perfoming queries against the context data the stored KV cache calculated earlier can be used instead of needing to rerun the data through the #acrshort("llm"). This system eliminates any retrieval latency that would be associated with a #acrshort("rag") based approach. It has been shown to consistently outperform #acrshort("rag") in performance, efficiency and scalability. This approach works best with documents of a manageable size that can fit inside the context window of an #acrshort("llm"). #cite(<chan_dont_2025>)

=== Agentic #acrshort("ai") and #acrshort("ai") agents
Agentic #acrshort("ai") is capable of goal-oriented, autonomous action and is adaptable. These #acrshort("ai")s are based on generative #acrshort("ai") such as LLMs. Unlike conventional LLMs they are not limited to the information they are trained on as they can do things such as independently gather new information. Like an LLM an agentic #acrshort("ai") can be interacted with through a natural language prompt, making them simpler to use than other software products. #cite(<noauthor_what_2025>).

According to #cite(<pounds_what_2024>) agentic #acrshort("ai") uses a four step process:
+ Perceive
+ Reason
+ Act
+ Learn

The last step called learn involves gathering data from the system in use, and using it to train future iterations of the model. This creates a feedback loop called the data flywheel.

The difference between agentic AI and AI agents is that AI agents refers to individual AI agents, whereas agentic AI is a system involving multiple different AI agents working collectively. AI agents are defined as having autonomy, task-specificty, reactivity, and adaptation. This means they can operate with minimal human intervention, fit only a narrow domain, and can respond to changes in the environment. Agentic AI meanwhile uses multiple specialized agents to perform different specicalized tasks that all contribute to a more complex high level goal. Agentic AI systems have features such as persistent memory that can be shared between individual agents. They can decompose tasks given by a user and distribute them to the appropriate agents. #cite(<sapkota_ai_2026>).
